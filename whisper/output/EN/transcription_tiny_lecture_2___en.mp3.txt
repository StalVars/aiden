  Hello and welcome to the Lecture Intelligent Agents in the Artificial Intelligence Course. In this lecture, we will look more closely at the model of the rational agent, on which we briefly touched in the introduction lecture. We'll go deeper into the interplay of perception, cognition and action. We discuss how agents can act in different environments and discuss properties that can be used to characterize environments. We also introduce several types of agents in their underlying architectures. We discuss the simple reflex agent, the model-based reflex agent, the goal-based agent, the utility-based agent, and the learning agent. The recommended reading is chapter 2 of our textbook. It covers all the material that I discuss in this lecture. The metaphor of the rational agent has been very important for the last 40 years of artificial intelligence research. A central aspect of intelligence is the ability to act successfully in the world. We are therefore interested in AI systems, which are successfully acting agents. In this course, we will answer questions such as, how can agents perceive the environment, in which they act? What does it need for an agent to act successfully in an environment? What does it mean to be able to execute actions? What does it need for an agent to be successful in different environments? So what is an agent? This video shows two agents as we know them from action movies. They are not only agents in the sense of a secret service. Angelina Jolie, acting as Mrs. Smith, does the three steps, which we require from any agent. She perceives Brad Pitt's car. Based on what she perceives, she anticipates the consequences of him driving in the explosion circle. She thus takes a decision to disarm the system. This decision leads to an action. She gets up and disconnects the cables from the control device. Thereby, she takes action to stop the explosion to happen. Summarizing, we can formulate the following properties of an intelligent agent. An agent must be able to perceive the environment. The perceptions must be used to take decisions, and the decisions will result in actions. If the agent is rational, its decisions must be rational. Being rational means that the decisions must lead to the best possible action the agent can take in the current situation. Being defined these four properties of the rational agent and we can now use them to build AI systems that can act successfully in the world. As a side remark on the slide, you also see a yellow star. Starting with this lecture, I begin marking especially relevant slides with these stars. Was the decision taken by Mrs. Smith a rational decision? Is Mrs. Smith acting here as a rational agent? Yes. By stopping the explosion, she saves the life of a person she considers to be not involved in her current investigation. Based on what she currently knows, she considers the man to be an innocent, idiot, as she says. She also saves the explosion equipment for further use to reach her original goal, which was to attack the approaching convoy of cars. Let us watch this short video sequence again to recapulate the behavior of the rational agent consisting of perception, decision, action. The name, agent, originally comes from the Greek word of agare, something that acts. A rational agent acts to achieve the best expected outcome. Rational thinking is a possible mechanism to achieve rational behavior. Perfect rationality cannot be achieved in complex environments, because we cannot anticipate all information that should influence our decision making and acting. Therefore, we consider a form of limited rationality. We consider an agent as being rational when the agent acts appropriately in a given situation having its limited resources available. Being rational does not mean that the agent knows everything about the world and the consequences of its behavior or the behavior of other agents. A rational agent is not an omniscient agent. Omniscient agents know the actual effects of all their actions and can perfectly foresee the future. This is of course not a realistic capability that would be within reach and something that we can build, as the future will always remain uncertain, and the world cannot be fully controlled. Therefore, a rational agent can only maximize the expected useful results or positive outcomes of its behavior. The rational agent behaves according to its perceptions from the environment. It may also have some background knowledge learned and acquired over time. Based on the percepts and possible prior knowledge, it tries to be in situations it personally finds useful for itself. For example, if I am hit by a meteorite while crossing the street, I can hardly be accused of lacking rationality. It's rather unusual that I'm hit by a meteorite and looking above into the sky as a rather uncommon behavior when crossing a street. But on the other hand, if I cross the street without looking left or right and get hit by a car, my behavior can hardly be considered as being rational as I know that cars drive on streets and may not be able to break immediately to avoid an accident. It certainly does not maximize my chance to safely cross the street when I do not look before I start crossing. Anality maximizes expected performance, in contrast, perfection maximizes actual performance. Here you can see the basic architecture of an agent. Any agent needs to have these elements shown in this architecture to be able to satisfy the three properties we define. The agent is equipped with sensors and actuators, and a white box element marked with a question mark in the middle. The agent uses the sensors to receive percepts from the environment. The internal white box element represents its capability to take a decision based on the percepts and then to select an action. With the help of its actuators, the agents can execute the action in the environment. When building intelligent agents in AI application systems, we need to decide with which sensors and actuators the agent will be equipped. What sensors does it need to receive important properties from the environment in its percepts and what actuators does it need to execute certain actions? Furthermore, we need to decide what is put into the white box element. What capabilities do we need to implement for the agent to reason and decide? Our decisions are based on our understanding of the environment and its complexity. We also need to define the boundary between agent and environment and the influence of agent and environment on each other. On this slide you can see a simplified model of a human agent. Humans have sensors such as for example their eyes and their ears. Or take the skin, through which we can also obtain information from and about the environment. We have our brain to reason about our percepts. The brain fills in the white box element marked with the question mark. We also have actuators such as our hands, our voice, and our legs for example. With these actuators we execute actions in the environment. Executing actions leads to changes in the environment, which in turn lead to new percepts that we can perceive through our sensors. Although we are agents too, we are not always rational agents. It is good that we are not always rational because this would otherwise lead us to maximize our own selfish interests only. Humans are often irrational, but also for the good of other humans. This is another example of an agent, the pepper robot. Pepper is a simple humanoid robot, which is commercially available. As you can see on the slide, it has various types of actuators and sensors. For example, it has a gyroscope, two-dimensional cameras, tactile sensors, a laser scanner, a sonar and it has safety bumpers, all these are sensors. As actuators it has, for example, loud speakers and wheels to move forward. It has two very simple arms and hands with which it can do some limited form of object manipulation. Here is another agent, the vacuum cleaner agent. This agent will follow us in this course, and we will discuss some aspects of its agent behavior in the environment, in which it works. Our little vacuum cleaner agent lives in a world that consists of exactly two rooms. The left room A and the right room B it has only two possible percents. It can find out if it is currently located in room A or in room B furthermore, it can detect if the room is dirty or not. As actions, it can move left or right, suck the dirt, or simply do nothing. The autonomous vacuum cleaners that we can buy today are results of AI research, but research that was conducted about 30 years ago. Some of these agents can only perceive very simple percents, for example when they hit an obstacle. They very often can't even notice dirt and they just wander around in a more or less clever way to clean the floor. Other commercially available vacuum cleaners can build a map of the environment when they start exploring their new home. Our agent is a little smarter, it detects dirt, but it does not have a map and its actions are only built for an environment with two rooms. How can we model agents such as the vacuum cleaner? Out of all, we need to be able to store the percents that the agent has received. We call this the Percept sequence. The Percept sequence is the complete history of what the agent has perceived so far, from the birth of the agent to the current moment in time. In this Percept sequence, all the Percepts are stored. Then we need an agent function. The agent function maps any given Percept sequence to an action. One the slide. We see pseudo code of the agent function called table-driven agent. Percepts function takes a Percept as an argument, and it returns an action. The agent function takes the current Percept and appends to the Percept sequence. It has some internal storage, where it can keep the Percept sequence, which is initially empty, but grows over time. Furthermore, there is a table of actions, where all possible actions are indexed by all possible Percept sequences. Using the lookup function, the agent takes the current Percept sequence and finds the action to be executed next in the table. This is a very simple form of an agent. It has a lot of memory to always return the action that fits best to the current Percept sequence. Finally, we need the agent program, which implements the agent function. It takes the current Percept as input from the sensors and returns an action to the actuators. This is an example of an agent table, in which we see the Percept sequences of the vacuum cleaner. The vacuum cleaner agent can have two Percepts, modeled here as a pair. In the first row, the agent is in room A, and the room is clean. The best action to perform is to move right to room B if the agent is in room B, and the room is clean. The agent wanders back left to room A in order to observe if anything has changed in the other room. This situation is depicted in the second last row shown in the table. If room A is now dirty, the agent sucks as shown in the last row of this table. The first four rows show all possible Percepts in the initial state of the agent when it makes its first Percept. The agent table must contain all possible Percept sequences, otherwise this simple agent does not know which action to execute. By storing all Percepts in the Percept function, the agent can take into account its past experience when determining the next best action. Of course, we can imagine more compact representations of this table for more complex domains. Furthermore, we can also allow the agent to forget old Percept sequences. This agent table implements the agent program. If the current square is dirty, then suck, else move to the other square. We are of course interested in the performance of an AI agent. We want the agent to execute the best possible action in a given situation. In order to evaluate the performance, we have to define a performance measure. The performance measure is evaluating the agent's behavior from the perspective of an external observer. We can imagine various performance measures for our vacuum cleaner. For example, measuring the square meters sucked per hour, the level of cleanliness that it achieves, the energy usage it consumes, if it is noisy or not, the safety of its behavior. We do not want this agent to suck in useful objects, or children that we like and animals that we want to keep. The best possible behavior is often very difficult to achieve. Not all relevant information is perceivable, and the complexity of selecting the best possible action could be too high. In fact, when it comes to real-world applications of AI algorithms, defining good performance measures is often a challenge. Different performance measures lead to a different behavior of the agent. To compute the performance measure, we have to take into account that each action of the agent takes the world to another state. If the sequence of world states is desirable for an external observer, the agent has performed well. Let's assume we sit on a chair and watch our vacuum cleaning agent. It does various actions. It moves from one room to another, and it sucks. Based on our performance measure, we observe the sequence of world states and then judge if the agent has performed well or not. The performance measure is independent of the agent. It is based on the perception of an external objective observer that only looks at the states of the environment. Otherwise, the agent could achieve perfect rationality and perfect performance by deluding itself that its performance was perfect. It is important to notice that we get the behavior that we reward. For our vacuum cleaner agent, an intuitive performance measure is to count the amount of dirt collected because this may relate to the cleanliness of a room. Unfortunately, if you think more about this, you realize that the agent could develop a rather unwanted behavior to maximize the amount of dirt it collects. It could suck and dirt and immediately release it if it has the capability to do so. If the vacuum cleaner does this repeatedly, it can maximize the performance without cleaning the rooms. A more aggressive vacuum cleaner could even begin to produce dirt to be able to collect even more dirt. This phenomenon is called reward hacking. Designing the right performance measures is thus critical for applications. The ideal rational agent depends on several factors for its behavior. The performance measure is related to its goals. It depends on the percept sequence that it can perceive from the environment. Active perception of the agent is important in real-world domains, for example to learn causal and physical relationships. This knowledge about the environment can be used by the agent to develop rational behavior. Finally, the behavior of the agent depends on the range of capabilities of executing possible actions. By definition, a rational agent selects the action that is expected to maximize its performance measure given the evidence provided by the percent sequence and would ever build tin knowledge the agent has. Let us now discuss how we can characterize the agent in its environment. We introduce so-called P's descriptions of agents. PEAS stands for Performance Measure, Environment, Actuators, and Sensors. In the picture you see the example of a pepper robot. The robot is in a station shop on a Swiss mountain called RIGI. The robot was programmed to answer questions of tourists. Its performance measure is the number of correctly answered questions. As actuators, the robot uses the loud speaker and as sensors it uses the camera and its microphone. The performance measure is used to describe the behavioral success of the agent. The environment is specified by giving key elements in the environment, which are relevant to the agent. Actuators describe the capabilities of the agent for executing actions. Sensors describe the capabilities that the agent has available to receive percepts. Here, we see more examples of PEAS descriptions. For example, of a medical diagnosis system or an interactive tutoring system for learning English. Each example agent is characterized by one or more possible performance measures. Furthermore, the table shows key elements of the environment and the actuators and sensors possessed by the agent. For example, to assess the performance of the medical diagnosis system, we could measure the number of correctly treated patients, its costs, or if there are any lawsuits issued against the system due to cases of poor diagnoses. The environment of the system comprises patients, doctors, and diagnostic devices. Sensors of the diagnosis system are the loudspeaker and the display that the system uses to communicate its diagnoses, ask questions, or recommend treatments. As sensors, the system has cameras, and it interacts with sensors of the diagnostic devices to which it is connected in order to receive information about symptoms. Furthermore, a keyboard and a microphone are available as sensors to receive various data inputs from humans about the situation of the patient. The textbook discusses more examples, and it also elaborates on different combinations of agents and environments, as well as P's descriptions in more detail. We have introduced the performance measure, which evaluates the behavior of the agent when observed by an external observer. But the agent itself needs to implement a behavior, goals, and a mechanism to select actions such that it can meet our performance expectations. For this purpose, the agent uses its utility function, which is its internal representation of our performance measure. The utility function is used by the agent to evaluate the desirability of a state of the world. The utility function maps a state or, if we take the history into account, a sequence of states, to an evaluation value. Similar to the performance measure, this value is represented as a real number, often in the interval between 0 and 1, with more desirable states receiving a higher utility value. A state evaluation, which is closer to 0, denotes an undesirable state, a state evaluation that is closer to 1 denotes a desirable state. The agent uses the utility value of a state to weigh the importance of competing goal states into select actions, which bring the agent closer to the desired state of the environment. Here we define the process of action selection more formally. The utility function combined with the percepts of the agent and any background knowledge leaves to the selection of an action the agent will execute next. A selected action is optimal if it takes the agent to a state of maximum expected utility given available percepts and knowledge. The agent is rational if it always chooses optimal actions. This slide shows an example of a possible utility function. The little pepper robot on Mount Riggie in Switzerland, called the Riggiebot, used Google Speech to text to transcribe questions into written text. Recognized questions were sent to a knowledge base built with Microsoft Q&A Maker. As utility function, the Riggiebot used the confidence values returned by these AI cloud devices. The state machine shown on this slide drives the behavior of this agent. Based on the confidence values, the agent decides between different states. When starting up, the Riggiebot does an initialization to connect to the external AI services. Then it gets into a ready state. Through its cameras it scans the environment. If it detects a person, it will introduce itself and stream a welcome message to explain that it can answer questions. Then it listens to what the human says. If speech is recognized with a confidence value above a certain threshold, this speech is sent to the speech to text service. If during listening an error is perceived, the robot goes back into its initial state. If the person is leaving while the agent tries to listen, it goes into a state where it disconnects from the listening action and from their back to the ready state to listen again. If the listening is successful, it tries to answer the question of the human. Depending on the confidence value returned with the answer from the AI Cloud services, the response of the agent is different. If it gets a low confidence back from Google speech to text, it says, please repeat your question in the hope that it can record the question better in the next attempt. If Microsoft Q&A Maker returns no answer or an answer with a very low confidence value, it says, I don't know. Otherwise, it responds with the received answer. Let us look at the environment of an AI agent. In the following, we discuss properties of environments in more detail. You see our little agent on the snowy mountain with the tourists in the background. This environment has different properties than the one the robot encounters in the station shop. The environment is unknown to the agent. The agent can only partially observe it because its sensors will not give the agent complete information about the environment. However, can the agent observe those properties of this environment such that it can successfully act in this environment? Think of the Alexa speech service in the Amazon Echo. It also lives in an unknown environment. It cannot observe the environment and things may happen that the agent has no idea about. Everything is very dynamic and other agents are around. Nevertheless, this agent can successfully answer questions and it can also interact with the environment by, for example, controlling devices in a smart home. Let's take a closer look at the properties that we can use to characterize environments by asking the following questions. Does the agent initially have complete knowledge about the environment? If the agent has complete knowledge because we programmed it with a complete environmental model, then we say the environment is known. For example, a chess playing agent usually has complete knowledge about the chess board, the pieces, how pieces can move, how it can assess the situation in a chess game, and while the game is evolving, the agent can also completely observe the progress of the game. If an agent does not have complete knowledge about the environment, we say that an environment is only partially known or even unknown. For example, if we drive to a city where we have never been before and we have not acquired any information about this city before our trip, then we are in an unknown environment. For the chess playing agent, the environment where the chess board is located is only partially known. The agent knows the chess board, but nothing about the room where it plays. The next question asks if the agent can observe all relevant aspects of the environment with its sensors. If this is the case, we say that the environment is observable, otherwise it is unobservable. The important thing to note here is that we focus on the relevant aspects of the environment. For example, for a chess playing agent it is important that it can observe the pieces on the chess board. But it's not important for this agent to observe the room temperature or to know the color of the shirt of the other player. The third question is about the changes in the environment. Our agent will take some time to execute its cognitive functions, it is deliberating and thinking about the world in order to decide for an action which it wants to execute next. Is this environment changing while the agent is thinking? If it is not changing, we have a static environment. For example, the chess game is static because our agent has enough time to determine the next move and the situation on the board is not changing while it determines this move. Other environments, for example an agent driving an autonomous car in a city, are rather dynamic. Finally, we ask, can the time, the percept, the actions of the agent be described with a finite set of discrete values? If so, the environment is discrete, otherwise we have a continuous environment. Being able to describe an environment as a sequence of discrete states is very fundamental for many AI algorithms. If this is not possible, we speak of continuous environments. For example, imagine a ball that you throw into the air, which will fly, and then land somewhere. We cannot model this action as a sequence of discrete states without losing a lot of detailed information. Discrete state models are always an abstraction of the environment. It is possible for some environments such as, for example, chess where we can abstract from the physical move of a piece on the chessboard. For other environments, for example throwing a flying ball, we need other approaches such as differential equations and continuous models. We can also use properties to characterize the effects of actions executed by an agent in an environment. If the effects of actions happen is planned by the agent, we say that the environment is deterministic, otherwise it is stochastic. Consider the following example. If the agent decides to move a piece in a virtual chess game, we can expect that exactly this piece will move to the new position. A chess game can be modeled as an environment with deterministic actions. If the agent rolls a dice, it cannot foresee the number this dice will show once it stops rolling. If the agent dies as a stochastic action, the possible outcome of this action can only be modeled using probabilities. The action of tossing a coin is also stochastic with two possible outcomes. If it is a fair coin, we don't know which side will show up after the coin landed, each of the two possible outcomes has probability one half. As another example, consider a road environment. Breaking on a dry road is deterministic. The car will stop after a given distance depending on its speed. Breaking on an icy road is stochastic. The agent cannot foresee for sure where the car will stop. The second action property asks if the effects of actions depend only on the current state, or if they also depend on the action history. Here we distinguish between episodic and sequential actions. In case of episodic actions, we do not need to remember the past to make good decisions. Episodic memory is another term for short term memory. The agent has the information about the current state and the effects of possible actions. By selecting an action, it can precisely determine the successor state without taking the history of previously executed actions into account. Sequential environments require long term memory in order to make good decisions about which action to execute next, because actions can have long term effects. For an agent to successfully act in a sequential environment, it must remember past decisions to take good future decisions. An example of a sequential environment is the chess game. The effects of the next move depend on earlier moves if we consider the chance of winning the game. For example, if a move leads to a position of pieces on the board, which threaten the king. On the other hand, if we only consider the action of moving a single chess piece, the environment is episodic, it simply moves the piece to another field on the board independently of other moves in the past. Another example of an episodic environment is an image processing system, which is classifying objects. Examples of sequential environments are driving a car or investing in stocks. Finally, we are interested to understand if there are several agents in an environment. If the agent can execute actions without the need to consider the effects of the actions of other agents, we speak of a single agent environment. If there are other agents acting in the environment, who in the agent needs to consider the activities of these agents, we speak of a multi-agent environment. Chess is a multi-agent environment with two agents. When deciding for a move in the game, the agent needs to consider the previous moves made by the opponent agent. This table summarizes the properties of simple and difficult environments. To sum up, simple environments are known. They are observable, static, discrete, deterministic, episodic, and we have a single agent. Simple environments are characterized by the opposite values of these properties. The key in designing successful AI applications is to understand how we can make environments simpler for the agent. What can we abstract from, and what is the key information, which we need to model in order for the agent to act successfully in an environment? We discussed that every agent needs an agent program. This program is implemented by an architecture. The architecture introduces various components that implement specific functions in the agent program. The architecture also defines the interfaces to the environment for the agent to be able to receive percepts and to execute actions. In practice, any agent architecture will have some limitations in terms of possible interfaces in its components using hardware and software. This implies that the agent will act with its limited resources and thus exhibit limited rationality. Let's take a closer look at different architectures of agents. Agents apparently differ in their capabilities. Some agents may be able to do exploration, which means they can execute actions to obtain more information about the environment. Some agents can learn, which allows them to derive additional insights from percepts. Other agents can also show different degrees of autonomy and improve partial or incorrect knowledge. We now look at five different types and architectures of agents, and we'll discuss example agents. The simple reflex agent, the model-based reflex agent, the goal-based agent, the utility-based agent, and the learning agent. Let's look at the architecture of the simple reflex agent. The simple reflex agent takes the sensor input, does some processing of the sensor input in order to determine the current state of the world, and then maps this information to a simple condition action rule, which determines the next action. There is no additional model of the world, no background knowledge, and no memory to remember the percept sequence. This simple type of agent can show behavior, which is very effective. We ourselves have simple reflex implementations. Just think of us touching a hot surface, we immediately remove our hand. Or think of the reflexes of our eyes. The lid's close when something comes near to them. No complex thinking by our brain takes place. In order to protect our body, we need a fast action response. Our reflex-based reaction comes immediately when we obtain the sensor signal. Here you can see the implementation of the agent function for a simple reflex agent. It receives percepts and maps them to a state. The state is matched against the condition action rules. The first rule, the condition of which is satisfied in the state, determines the next action for execution. We call these condition action rules also productions, because they immediately produce behavior. A simple example is if car and front is breaking, then break. Don't think but just act, because you otherwise crash into this car. Simple reflex agents can be very effective. If you think of the autonomous vacuum cleaners that we can buy today, most of them are simple reflex agent. They drive around and if they bump into some obstacle, they change the direction of driving and continue to work in another direction. The choice of action of such an agent is based on the current percepts only. If the environment is not fully observable, infinite loops can occur where the agent does not notice any change in the world state and repeatedly executes the same action again and again. Using a randomization in the choice of action can implement a simple workaround. Instead of always repeating the same behavior because the mapping returns the same state, the agent can sometimes select actions randomly. For example, our vacuum cleaner agent with two clean rooms will move between room A and B forever. We can improve this agent's behavior by simply adding a coin flip. Depending on the outcome of the coin flip, it will move to the other room or stay and do nothing in the same room. This video shows simple reflex behavior in the pepper robot. Pepper has a built-in capacitive sensor in its head. If you touch the head, it will react to the touch in different ways, which is an example of a simple reflex with some randomization. The simple reflex agent can be improved by adding an internal model to this agent, in which the agent can store the percept history and other information. The model can provide information about the effects of agent actions, which allows the agent to predict how the world will change after it has executed an action. Additionally, the model may contain information that allows the agent to predict how the world might evolve further in the future. Nevertheless, there will always be uncertainty about the precise state of non-trivial environments. The model is an abstraction containing the agent's best guess about how the environment behaves. Here we see the architecture of the model-based reflex agent. The model allows the agent to implement a more sophisticated analysis of the sensor input where it can apply the knowledge stored in the model about the state of the environment, how the environment evolves, and about the effect of actions. The world model helps the agent to develop a different assessment of the current situation of the world compared to the simple reflex agent. To choose the next action, the model agent also uses condition action rules only, however, its computation of the condition in which it finds the environment to be, is different, and hopefully better, when compared to the simple reflex agent. The agent function for the model-based reflex agent is shown on this slide. The function has access not only to the rules, but to a state representation of the environment, the model, and the most recent action. The interpret input function of the simple reflex agent is replaced by an update state function accessing the persistently stored additional information. The rule match function stays the same, but the model-based reflex agent can exploit a more sophisticated state representation. TH first matching rule is selected and determines which action is executed next. The state interpretation capabilities of this agent are more sophisticated. It is easy to implement a world model in the pepper robot given its basic awareness capability. Here, I implemented a model-based reflex agent where the model filters the information returned by the basic awareness component and the pepper robot then replies with an animated greeting when it detects a person. This video shows the resulting behavior of the robot. If a person comes close to the robot and the robot recognizes the person, it speaks a greeting. We can extend the architecture of the model-based reflex agent to obtain the utility-based agent. This agent has an additional utility function. It does not only compute the next state that it can achieve with its actions, but it also assigns an evaluation to the state. Depending on the utility of the state, it decides which action to take. This agent looks at the possible actions it can execute and then takes the best possible action. A utility-based agent therefore makes a rational choice towards an optimal action. In this video, we see a pepper robot that wants to drop a small box into a big box. The utility of the states of the robot, its position, is evaluated depending on the position of the small box in relation to the big box. If the robot succeeds in dropping the small box into the big box, the most desirable state is achieved. It tries to go to positions, which bring itself closer to the big box and eventually to a position where it can hopefully drop the small box into the big one. If you watch the video carefully, you see how it makes a corrective move to increase the chance of dropping the little box successfully. Here we see the architecture of a learning agent. You can see that the architecture changes quite a bit compared to the previous architectures. The agent has four key components, a learning element, a critic, a performance element, and a problem generator. The sensor inputs are mapped to the performance element and the critic. The critic gives feedback to the learning element about how good the agent performs in the environment. The learning element can change and improve the performance element based on this feedback. The performance element takes the sensor input to determine the next possible action. It can also provide the learning element with knowledge about the utility of states and thus performance, achieved by the agent so far. Connected to the learning element is a problem generator, which proposes actions, which put the agent in different situations to learn something new. The learning agent is a very smart agent, much more intelligent than the previous ones. It can acquire new skills and reflect on its own performance to improve over time. A very simple variant of the learning element is shown on this slide. This element enables a pepper robot to learn new faces and link the names of people to these faces. When pepper recognizes the face again, it will greet the person with the name it has learned. The resulting behavior of the robot is shown in this video. A successfully learning agent becomes more competent over time. It can start with an initially empty knowledge base, which means it knows nothing. It can operate in initially unknown environments, but step by step, it will build up a model of the environment by trying to solve problems generated in the problem generator component. Learning agents improve their behavior over time even if they can start with an empty knowledge base and operate in initially unknown environments. By learning, they add more knowledge to the knowledge base, including knowledge about their environment and even their own skills. This slide summarizes the responsibilities of the various components in the architecture of the learning agent. The performance element tells the agent how well it succeeded in the environment. The learning element improves the performance element because the agent learns, which actions lead it to more useful states. The critic evaluates the behavior of the agent based on its performance and gives the evaluation as feedback to the learning element. The problem generator suggests actions that will lead to informative experiences for the agent. The most sophisticated agent architecture is the goal-based agent. This agent sets goals for itself that it wants to achieve in the world. It then uses its world model to select actions to achieve its goals. The goal-based agent differs from the model-based reflex agent only in the goals component, which replaces the former simple condition action rules. Note that the goal-based agent is looking for actions to reach a state in which the goal is satisfied. However, any action sequence that leads from the current state to a goal state is a solution for the agent. To distinguish how good an action sequence or a single action is, the agent needs a utility function to compare the utility of different states and actions or to chose among competing goals. The goal-based agent does not learn unless we combine this architecture with components defined for the learning agent. In this video you see the spot robot from Boston Dynamics having the goal of getting to another room. The door to this room is closed. The robot has no arm to manipulate the door. It calls another robot, which is able to open the door. Here we see a multi-agent environment where two robots cooperate to achieve a goal. One agent asks for the help of another agent. Once the called robot has opened the door, both robots can go through the door and enter the other room. I would like to make a comment on the presentation of this video by the press. You see a comment at the end of the video that expresses how frightened people are by the technology shown. Such concerns must be taken seriously when we develop AI application systems. Technology by itself is not scary or evil, but some applications are. Of course, the design of the robots in the video is a scary, and that's why we get scary associations with the technology. But it's not the technology itself, which is positive or negative. It's rather the business model and the application in which we are using it. It depends on us what we do with AI technology and if we use it for something positive or something negative. With the capabilities shown in this video, I can imagine very useful applications. Just think about a person in a wheelchair. People in wheelchairs cannot open normal doors, and if those people had a little robot dog that can help them open doors, for example, this could be more than useful. With this slide we come to the end of this lecture. The metaphor of the rational agent is very fundamental to AI research. The rational agent maximizes expected outcome of its actions. Applications are not behaving as rational agents, which is good. We can also ask if AI agents should always be rational, or if they shouldn't behave for example in an altruistic manner. To successfully build AI applications, we must be able to characterize environments and successfully design the boundary between an agent and its environment. More complex environments usually require more complex agents equipped with complex sensors, actuators and intelligence, which are also able to learn. In this lecture, we looked at five different types of agents and their underlying architecture. We also saw from the videos that ethical and risk considerations are very important when we designing AI agents that autonomously act successfully in different environments. An interesting website is the MIT Moral Machine Experiment, which asks questions about accident situations of autonomous cars and motivates us to rethink our moral beliefs. I put together some working questions for you to repeat the content of this lecture. We looked at the concept of the rational agent and discussed its properties. We then learned about properties that we can use to characterize environments and learned about the P specification that we can use to describe the agent, its environment, the actuators and sensors, and the performance measure of the agent. We also discussed different agent architectures and the different capabilities that they provide to an agent. With this, I want to end this lecture. Thank you very much for listening.
