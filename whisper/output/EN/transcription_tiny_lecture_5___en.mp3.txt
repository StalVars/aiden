  Hello and welcome to the lecture on local and stochastic search in the Artificial Intelligence course. In this lecture, we learn about challenges such as local extrema and plateaus when searching very large search spaces. We learn about techniques to address these challenges such as randomized and stochastic search strategies. We will introduce several search algorithms that use randomized, stochastic, or local search techniques. In this lecture, we will also go further and introduce one of the biggest successes in search algorithms in artificial intelligence, namely Monte Carlo tree search methods and in particular the so-called UCT algorithm. Relevant scientific papers that give more details on stochastic search are listed at the end of this slide. Let us first look at the challenges of very large search spaces and techniques for how these challenges can be addressed. Let us look briefly at a practical search problem where we are interested in computing how a cable tree can be automatically wired by a robot. Such a machine cuts and prepare cables, and then uses a robot arm that takes the cable ends and plugs them into cable housings. Cable trees are used everywhere to transmit energy or information in devices. Many car and airplane manufacturers are customers using these machines. Most of the cable trees are however still manufactured by hand, but the trend is to make the cables thinner and miniatureize the housings to save material and weight. This makes it increasingly difficult or impossible for humans to perform the wiring operations. In a cable tree with end cables, two end cable ends must be inserted into housings, which leads to two end factorial potential insertion orders described by the possible permutations of the cable ends. Only some of these insertion orders are permutations of the cable ends or valid solutions allowing the robot arm to operate safely and among the valid solutions, only a fewer optimal, allowing the robot arm to work as fast as possible. If we only have two cables, there are 24 potential solutions. If we have 40 cables, which is very common in industrial cable trees, then we already have to deal with a search space of 10 to the power of 120. For search spaces of this size, there is no chance to systematically or heuristically explore them in reasonable time. A local search algorithm starts somewhere in the search space, uses an evaluation function for each node, and moves towards better evaluated neighboring nodes. Local search algorithms are suitable for problems in which only the solution state matters, not the path cost to reach it. Large search spaces contain a huge number of states, and within these states we find local maxima and minima. The local maximum is a node in the search space where all neighbors have a worse evaluation. When only moving towards better evaluated states during search, a local search algorithm will be trapped in the local maximum and return a suboptimal solution without being able to continue search towards better solutions. This slide shows some illustrations of local maxima in search spaces. In general, large search spaces have structures, which are challenges for local search algorithms. Besides local maxima, many search spaces contain so-called plateaus, shoulders, or ridges. A plateau is a large region of neighboring nodes in the search space where all nodes have the same evaluation. This makes it extremely difficult for an algorithm to decide how to proceed, because all nodes look equal having equal evaluation. The evaluation function does not provide the algorithm with information that enables the algorithm to find orientation in which direction a solution can be found. To escape a plateau, the algorithm has to move to worse evaluated node. A shoulder is a plateau from where a better evaluated node can be reached, and the algorithm can make progress towards a suboptimal or optimal solution. A ridge is a sequence of local maxima that are not directly connected to each other. All neighboring nodes of the local maximum have a poorer evaluation, so the algorithm either stops or must move to a node with a worse evaluation. But if it then starts moving towards better evaluated neighbors from this worse node, it will again end in a different local maxima with the same evaluation as the first one. There are techniques, which help a local search algorithm to escape from these structures. One possibility is to implement a so-called taboo search. During this search strategy, the local search algorithm remembers previous moves. It keeps a list of so-called forbidden states, which were visited before, and which helps the algorithm to avoid moves that lead to previously explored regions of the search space. In a short-term memory version, local search does not reverse the last move. Imagine the algorithm searches a big plateau where nodes have equal evaluation. So, from the current node, it picks an arbitrary neighbor. The search evaluation doesn't get better and now it has to decide where to go from this neighbor. It could happen that the algorithm accidentally decides to go right back to the node that it came from and end up in a loop where it iterates between two nodes. With short-term memory, we invest a limited amount of memory to avoid such moves. Further, local search can do something like a random walk. We inject so-called noise and the algorithm jumps to a node in the neighborhood of the current node with some selection strategy based on the noise, for example by picking a node with a worse evaluation with a certain probability in each step of the search. Another important technique is a random restart. Using random restarts, the algorithm will start over from the initial state or some random state once it either runs into a timeout, finds a local maximum, or when another termination condition is satisfied. It thus performs many local searches and then picks the best solution that these different searches have returned. But theoretically, random restarts lead to a complete search strategy because eventually local search using random restarts will find the optimal solution when it is given sufficient resources. Let us now turn to different algorithms of local and stochastic search. Let's look at the first local search algorithm, which is called hillcliming. This algorithm is also known under the names of steepest-assent search or greedy local search. Hillcliming starts in the initial state and expands in each step the successor node that has the highest evaluation value. It will stop when no better evaluated neighboring nodes exist and return a local maximum. The local search algorithm is very well suited to search in the solution space. In many applications, we do not care about the path to the solution. For example, if we look at the eight queens problem, we are only interested in the correct placement of the eight queens on the chessboard such that the queens cannot attack each other. How we constructed the solution doesn't really matter. In the cable tree wiring problem, we look for a safe and fast permutation of cable ends, but we do not care about how the permutation was found. In the vacuum world, we need a plan that cleans all rooms, how we constructed this plan does not matter. This means that starting with a randomly generated and perhaps partial solution is a good idea. The local search algorithm can be used to improve the solution step by step. Let's look at an examples of how the hillcliming algorithm works on the eight queens problem. A state is a distribution of all eight queens on the chessboard. The criterion for a valid solution is that the queens cannot attack each other. If you remember how queens can move on the chessboard, then you immediately know that to achieve this condition, two queens can never be in the same column, because otherwise they could attack each other. So we have to place each of the eight queens in one of the eight different columns on the chessboard. You can see the queens being placed randomly in the different columns in the picture on the right. Each arandum placement will usually lead to a state where some pairs of queens attack each other. We will use the number of queens that can attack one another as our evaluation function h. A move in the search space means that we select a column and we move the queen in this column to another field in the same column. So during search, the algorithm essentially moves queens up and down. In the examples state shown here, there are 17 pairs of queens that can attack each other. Using a queen to another field in the same column generates a successor state. The best successors have an evaluation value of 12. Hill climbing will choose randomly among these best successors. Let's look at another evaluation function and its use in hill climbing. Instead of counting the number of attacking queen pairs, we count the attacks or conflicts into which a single queen is involved. A conflict is the number of potentially attacking queens when a queen is in a specific field. Let us consider the rightmost column and where we can move the queen currently located in the lower right cell. In the current cell, this queen has one conflict with the queen in the middle of the chessboard depicted by the red line. Note that there are no other conflicts remaining between the queens on this chessboard. We need to move the queen in the rightmost column to another position, but there is no position without a conflict. The same is true for the other queen involved in the conflict. All neighboring states either have one or more conflicts left and the algorithm is stuck in a local minimum. The eight queens problem has 10 to the power of 14 random distributions of queens on the board. Testing hill climbing on randomly generated distributions it will on average need four steps to find a solution or three steps to end in a local minimum where conflicts remain and from where knock solution can be found. Hill climbing can only solve 14% of all eight queens problems will get stuck on 86%. Hill climbing could try the following escape strategy. It randomly picks one of the two equally evaluated positions. In the picture in the middle of this slide, we can see that the algorithm picked the upper best position for the queen in the rightmost column. There is again one conflict left, which can be resolved by moving the queen in the third column from the right. If the algorithm evaluates the conflicts of this queen, it will find one move leading to a field in the lowest row on the chessboard, where this queen has no conflict. This move leads to a solution of the problem as now all columns have no conflicts left. The search terminates in the global minimum. The escape strategy of the last slide introduces a random walk. Using random restarts and random walks on the eight queens problem can dramatically improve the success of hill climbing on this problem. We saw that the standard hill climbing algorithm has a success rate of only 14%. Now take one divided by 0.14, and we see that we need approximately seven restarts to solve an instance of the eight queens problem. The algorithm will probably fail six times out of the seven, but one time it will very likely succeed and find a solution. With seven restarts, hill climbing can solve very large instances of the problem using end queens. On an enlarged chessboard with up to three million queens, it finds a solution in under one minute using a random restart strategy. When allowing the algorithm to use up to 100 ar random walks to a state with identical evaluation, hill climbing can solve 94% of all instances. An average run of the algorithm will require 21 steps until it finds a solution or 64 steps to end in a local minimum. The lesson to take away here is that successful local search algorithms require us to combine some elements of randomness with a strategy to follow the heuristic. The algorithms combine so-called exploration where they simply do something they haven't tried before in which might be even discouraged by a heuristic, with phases where they systematically follow the heuristic evaluation of a node and search areas in the search space where the heuristic values of the nodes are getting better. This is a fundamental principle that we will also see exploited in other algorithms discussed in this lecture. The soundness of hill climbing depends on how the problem is modeled, because a state may also represent a partial solution only. Such a partial solution may be useful or not depending on the application. Obviously, the basic version of the algorithm is neither complete nor optimal. However, when using random restarts, hill climbing becomes complete and optimal. The time complexity is constant when a predefined number of steps is used. The algorithm will exhaust this time limit if no local maximum is found earlier. It can run forever when no better evaluated neighbor is found, for example on large plateaus or in infinite state spaces. That's why setting a time limit is more than advisable for local search algorithms. The space complexity linear and at most of order B depending on the branching factor in the search space, when the algorithm keep the current node and its neighbors in memory. It can be reduced to a constant two, when only the current node and the next investigated successor are stored. When visited nodes are stored, space complexity can be become exponential if not bound otherwise. The second algorithm, I want to briefly talk about is called simulated annealing. This name takes inspiration from the cooling down of processed metals. A temperature-capital T, which gradually decreases during search, controls how often successor states with a lower evaluation are accepted. In contrast to hill climbing, this algorithm selects a random successor, not a successor with higher evaluation. If the random successor node has a better evaluation and thus the delta between the evaluations of the current node and the successor node is smaller than zero, the successor node is always accepted. If the evaluation of the successor node is equal or lower, it is only accepted with probability e to the power of minus delta v times t. Node that t is the step counter of the algorithm. The temperature-capital T is set to 1 divided by t and thus slowly reaches zero and then the algorithm terminates. Using the e-function with a negative exponent helps us to implement a smooth decrease as the curve in the upper right corner shows. This animation shows how the probability of accepting neighboring nodes with lower evaluation evolves over time. The curve shows the evaluation of nodes and the red line shows which node the algorithm selects. We also see how the temperature decreases during search. In the beginning, the algorithm jumps around a lot, but with a decreasing temperature, it converges more and more to a certain region where it finds the optimal solution. The soundness of simulated annealing is the same as with hill climbing and depends on how the problem is modeled, because a state may also represent a partial solution only. Such a partial solution may be useful or not depending on the application. In practice, simulated annealing is incomplete and not optimal, but if the temperature is lowered slowly enough, it can be shown to converge to the global optimum with a probability reaching 1, which means that it has a very high chance of finding an optimal solution. The time complexity is constant and determined by the number of increments by which the temperature is lowered until it is sufficiently close to zero and the algorithm stops. The space complexity is constant and two, because the algorithm needs to keep the current node and one randomly generated neighbor in memory. When visited nodes are stored, space complexity can be become exponential if not bound otherwise. Let us first look at Monte Carlo algorithms. The idea of Monte Carlo algorithms dates back to the 1940s. They were developed to run computer simulations for risk analysis when nuclear technology was developed. The basic idea is that the algorithm simulates a large number of random samples, and as the number of samples grows and grows, the average sample mean of unknown parameters will approach their actual mean. This principle is formulated in the law of large numbers. Let's look at an intuitive example of the law of large numbers. We are interested in determining the number pi. A possible method based on the law of large numbers is to observing raindrops falling on a cardboard. Rain is a random process. We draw a circle on this board and count the number of raindrops that fall on this board. Some of these drops will fall inside the circle. Others will fall outside the circle and in the square. Note that the area of the circle divided by the area of the square is equal to pi over four. We do not need to know the actual size of the circle and square. We count the number of raindrops that have fallen in the circle and the square and calculate the number pi based on the formulas shown on the slide. The growing number of raindrops, our calculation of pi will get better. Over time, it will lead to an increasingly better approximation of the value for the number pi. How can the law of large numbers be implemented in Monte Carlo tree search? Monte Carlo tree search is a method for finding optimal decisions in problem by taking random samples in the decision space and building a search tree according to the results of the samples. MCTS works as a statistical anytime algorithm. The algorithm will return a result within any time, but if the algorithm has more time available, it will produce better results. Interestingly, this algorithm needs only very little or no domain knowledge because it is making random choices. However, by making these choices, the algorithm develops an understanding about what is a good and bad choice, and its choices get better over time. Since the 1990s, the ideas of Monte Carlo tree search are applied to game playing and other search problems in AI. Today, it is the method of choice for very large search spaces and was for example instrumental for Alphago to master the game of Go. Many variants and improvements of this search algorithm exist. The basic MCT process build a search tree in an incremental and asymmetric manner. In each iteration, the algorithm uses the so-called tree policy to determine the next node to expand. The algorithm picks this node, here depicted in blue, and from this node it runs a stimulation by randomly executing actions. Such a simulation is also called a random rollout. This simulation leaves to some terminal state, which can be a goal state, a dead end or any other state. For example, a state at a predefined depth, in which no further actions can be executed. This state is evaluated yielding the reward obtained with the sequence of random actions. The reward is propagated up the tree and influences how the algorithm continues the search based on the tree policy used by the algorithm. It balances exploration with exploitation. The algorithm will continue to explore unknown areas of the search space, but will also search more intensively in areas yielding good rewards. The MCTS algorithm needs to decide which node to select for the next rollout. This decision problem belongs to a wider class of problems, which are called bandit problems. A bandit is a slot machine into which a player inserts money and then pulls the lever for a certain time starting the bandit to play. After the bandit stops, a configuration of pictures will appear on the screen, which decides about the win or loss of the player in this game. For a bandit with several arms, the player needs to decide which arm to play. Suppose we have pulled the right lever and won some money, so pulling the right lever looks promising and we are inclined to pull it again. Let us assume, we instead pull the left lever and win even more money than when pulling the right lever. The left lever now looks even more promising than the right one. However, we don't know whether this is true because we have no idea how the bandit machine works and what the underlying reward distribution is. Which arm should be pulled in each step of the game to maximize the win? A very effective algorithm to answer this question is UCB1. Let us now look at the four phases of each iteration of an MCTS algorithm. In the selection phase, the algorithm chooses which branch to explore next after it has tried at least one rollout from each successor node of the current node. In the expansion phase, the algorithm expands the tree towards an unexplored child of the selected branch. In the simulation phase, the algorithm executes a rollout to a terminal state to obtain a reward. Finally in the back propagation phase, the reward is used to update the value information of nodes higher up in the tree towards the root node. Selection and expansion are controlled by the so-called tree policy. The rollout is controlled by the so-called default policy. Let us now look at one of the most successful search algorithms for stochastic search, called UCT. UCT was published in 2006 and made huge breakthroughs possible, in particular for playing games with large search spaces. UCT combines two ideas. First it uses a search technique known as Monte Carlo tree search. Second it exploits a technique to select promising parts of the search space called upper confidence bounds. The combination of these two names also led to the name of the algorithm. UCT stands for upper confidence bounds applied to trees. On this slide, we see the main phases of the UCT algorithm in pseudocode. The algorithms starts in the root node S0 of the tree. Two important parameters are initialized with 0, namely the so-called end value, which is the node count how often a node was visited, and the so-called Q value, which is the sum of all rewards of simulations from the root node. As long as the algorithm has not exhausted its computational budget, it will iterate between its four phases and select a node and expand this node based on the tree policy, then run a rollout based on the default policy, and back propagate the reward up to N0. When the computational budget is exhausted, the algorithm will stop and return the action to be executed in node S0 leading to a child node of S0 having the highest Q value. UCT uses UCB1 as its tree policy. The select function within the tree policy is computing the UCB1 value of a child node as the sum of two mathematical terms. The first term encourages the exploitation of higher reward choices and the second the exploration of less visited choices. The first term computes the sum of all rewards that have been collected from rollouts through and wherein as a child of N and divides this sum by the number of visits of the node N. This yields the average reward the algorithm has obtained by running simulations through an so higher rewards in fewer visits make this term larger. The second term contains a constant C multiplied with another term. The lower constant C decreases the amount of exploration. The larger value of C increases the amount of exploration. The square root in the term is encouraging the exploration of less visited choices. We take two times the logarithm of the visits of the parent node N and divide it by the number of visits of the child node N. If the node N has been visited less, then this term will become larger. As a result, the child node N will receive a higher UCB1 value and is more likely to be visited again. Now let's look at the tree policy. As long as the goal test as false and no terminal state was reached, unexplored children of N are expanded. In the expand function, an arbitrary action from the set of all untried actions in state N is chosen, and the child node reached by applying this action is generated. If all children have been explored, the algorithm selects the child node with highest UCB1 value using the select function. This means, it can make a more informed choice about which child to select because they all have been explored. The simulation phase is controlled by the default policy. The default policy will choose an action uniformly at random and generate the child node until it reaches a node with a terminal state. The result of the default policy will return the reward obtained in this state. A default policy can also be more sophisticated and select actions based on data gathered from earlier rollouts. Note that we do not need to evaluate the intermediate states between the selected node, from which the rollout starts, and the terminal node. Once UCT has received a reward, it is backed up to the nodes in the search tree. During backpropagation, the N value and the Q value were updated for each node along the path from the selected node N back to the root node of the search tree. root values can be discrete or continuous and are usually normalized to the interval 0, 1. The N value is the visit counter of Anode and incremented by 1. The Q value of a node is the sum of the rewards, so the new reward is added to this sum. This slide summarizes the UCT algorithm with its four phases. You can download the complete description of this algorithm from the CMS. On the following slides, we will work through an abstract example run of UCT. This slide introduces a game situation, which can help you to follow the example computations on the next slides. Suppose we have an irregularly shaped chessboard. A pawn is standing on an upper right field of this board in the initial state. In each move, the pawn can either move one field to the left or one field down. Note that it can never move up a field up or a field to the right. The pawn is trying to reach the goal state at the bottom left of the chessboard marked with green color. A terminal state is reached when a downside wall is hit and the move down action cannot be executed anymore or the goal field is entered. In this case, a reward can be calculated staying how far this state is from the goal state. Now imagine the pawn tries different sequences of actions, initially just going down in each step and then also taking some sidesteps. After several such attempts, it knows which sequences of moves bring it closer to the goal state. This is the basic idea of the UCT algorithm. As a reward function, the pawn uses the Manhattan distance. The reward is calculated by normalizing this distance where we assume that the maximum Manhattan distance on this irregular chessboard can be 50 fields. The goal state has maximum reward one. All other states have a lower reward. Let us now look at an example run of UCT. The algorithm starts by selecting and creating the root node, corresponding to the initial state. Both the N and Q value of this node are set to zero. Now UCT expands this root node. There are still unvisited children of the root node. UCT randomly selects one of the untried actions A1 or A2. Let us assume that action A1 is selected. Action A1 leaves to the child or successor state S1. UCT then generates a new child node N1 corresponding to this state S1 with N and Q values of this node equal to zero. UCT moves to the node N1 with state S1. UCT runs a simulation from this node, so it tries to reach a terminal state Sg by a random play out of actions. Here you see that it is very important that at one point the simulation terminates. Otherwise, the algorithm would get stuck in the simulation and not work. After some time, UCT reaches a terminal state Sg and receives a reward. Let us assume of value 0.2. Now UCT backpropagates this reward up to the root node N0. It goes back to the node N1 that it has selected and from their update the Q and N values. The Q value of N1 is 0.2 and UCT visited this node once, so its N value is 1. UCT continues the backpropagation up to the root node N0 of the search tree. This node has been visited once and it also receives the Q value of 0.2. UCT is now at the root of the search tree again. The slide shows the current search tree. The first iteration is completed and UCT starts the second iteration. UCT again expands the initial state because it still has unexplored children. This time, it selects the last untried action A2, which is applicable in the state S0, and generates a new node N2 corresponding to the successor state S2 with N and Q values set to 0. UCT moves to the node N2 with state S2. UCT again runs a stimulation, this time from node N2. It reaches a terminal state Sg and receives reward 0.1. Now backpropagation follows, UCT propagates the reward of 0.1 back up to the root node. node N2 receives a Q value of 0.1 and its visit count N is increased to 1. At the root node N0, the Q value is incremented by 0.1 to 0.3. The visit count of the root node N0 is incremented by 1 and now has an N value of 2. After the second iteration the search tree looks as shown on this slide. UCT proceeds with the third iteration. This iteration is different because there are no unexplored children of the node N0 left. Both actions that are available for application in state S0 have been tried. But now, the UCB1 values can be computed for the nodes N1 and N2 and the selection phase can be entered. You see the formula for the UCB1 value at the bottom with the choice of constant that we insert into this formula. You can also see that the two UCB1 values are slightly different because the reward that UCT received in the terminal states was different. UCT now selects the node with the better UCB1 value, which is N1. UCT now expands this node. It has two possible actions again, a 1 and a 2 applicable in state S1. None of these actions have been tried, so UCT uses the expand function and simply chooses one of the untried actions to apply in state S1. Let's assume it picks action A1. This generates the child node N3 with state S3 and N and Q values both set to 0. UCT now moves to the node N3. UCT runs a simulation from this node. The simulation eventually ends in a terminal state. Suppose that this time the reward is 0. UCT back propagates this value back up the search tree. This time the reward is equal to 0 which means that the Q values don't actually change. The visit counts, however, are incremented by 1. This slide shows the search tree after the third iteration. UCT is back at the root node of the search tree. There are no untried actions at the root node. UCT now computes the UCB1 values for the child nodes N1 and N2 are the root node again. This time the UCB1 value for the node N2 on the right-hand side is larger. The UCT algorithm will hence stop to explore the subtree starting in N1 for now. This is because the last rollout didn't increase the reward, so the left subtree doesn't look so promising anymore. Instead, the algorithm will continue on the right-hand side. It selects the node N2. This assume that in this node there is only a single applicable action A1. UCT executes this action and generates the child node N5 with N and Q values equal to 0. UCT now moves to the node N5. The algorithm runs a simulation from node N5 and reaches a terminal state sg with reward 0.1. Now UCT back propagates this reward again up to the root node. The Q values of all nodes up to the root node increase by 0.1 and the visit counts and of the nodes along this path are incremented by 1. After the fourth iteration, the search tree looks like this. UCT needs to make another round of selections because all actions executable in the root node state have been tried. The UCB1 values of the child nodes of the root node are updated based on the reward obtained from the last simulation and the increased number of visits. For both nodes N1 and N2, it obtains a value of 1.77. UCT has encountered a tie that it needs to break because the UCB1 values of the child nodes of the root node are identical after the four simulations done so far. It needs to select a node based on some strategy for how to break ties on identical UCB1 values. In our case we assume a very simple strategy. It takes the leftmost node. This choice leads to the exploration of N1 again. There is still an unexplored child of N1, reachable through the yet untried action A2. The action A2 generates this child node N4. The node N4 is expanded by applying the action A4 and NNQ values are set to 0. UCT runs a simulation starting in node N4 and reaches a terminal state with reward 0.3. Once again, the reward is backpropagated along the path from N4 via N1 to the root of the tree. For each node along this path, the Q values are incremented by 0.3 and the visit counts by 1. This is the search tree after the fifth iteration of UCT. Suppose that UCT has now reached its computational budget and the algorithm terminates. You can see that the algorithm explored the left-hand side of the tree one more time than the right-hand side, and the accumulated reward is also higher on this side. The algorithm will now return the best action to take from the initial state S0 based on the knowledge that it gained through the rollouts. There are several strategies of how to define the best action. A possible strategy simply selects the action that leads to the child node with most visits. This is a good strategy because when UCT visits a node more often, this also means that this node looked more promising than the other choices. In our example, this strategy lets UCT select action A1 for execution leading to node N1. In a new search, this node would become the root of the next search tree to allow UCT to determine the next action. UCT always only determines the next possible action and not an entire solution in a run of the algorithm. In problems with few terminal states, some simulations will not terminate and a depth for the rollout must be set. Theoretically, the algorithm is complete nor optimal as convergence to an optimal action sequence with probability reaching one can be shown when the computational budget is infinite. UCT has constant time complexity, because it stops when it encounters a time limit. If no time limit is set, it can have exponential time complexity, because it will run until it exhausts the search space. In the worst case, UCT will need exponential memory when it builds the entire search tree. However, in practice, memory consumption is limited by the depth of the rollout and can even be bound by a constant. Let us now look at a new class of algorithms, so-called genetic algorithms. So far, all search strategies have selected one specific node corresponding to a single state, looked at the applicable actions, and then generate possible successor states. Genetic algorithms work very differently. They are based on the idea to select two solutions as parents and recombine them into a new offspring solution. The basic idea of these algorithms is based on evolution. Evolution seems to produce individuals, which are well adapted to their environment. Thus, researchers asked if evolutionary principles can also be used to find good solutions for a problem. Genetic algorithms try to simulate evolution by imitating sexual reproduction. They take two parents and combine their genomes by a crossover operation. They can mutate single genes, and selecting sufficiently good individuals for the next generation of the population. To implement a genetic algorithm, we need to encode a partial solution as a gene, which is often represented as a string. A string representation makes it easy to implement all reproduction operations. We need to define a fitness function to evaluate how good an individual is. The fitness value is then used to select good individuals from the population for evolution. Resulting offspring individuals with good fitness values are maintained and the next generation of the population is generated by also discarding some existing genes. This slide summarizes the main operations of a genetic algorithm. In population of individuals, the algorithm select a few of them, and then does a pairwise recombination trough a crossover operation. It takes their gene representation, decides to cut at a specific position, and then applies a crossover to obtain new individuals. Finally, in some individuals, specific genes are mutated. Based on the fitness function, the algorithm updates the population and produces a new generation from which evolution starts again. You can see that these algorithms offer many variation possibilities, for example how you select individuals, how to implement crossover, in particular, where to cut, or at which rate to do mutations. Let's look at how a genetic algorithm can solve the eight queens problem. The distribution of the queens on the chessboard is encoded in a string representation. Each string contains a sequence of numbers, and the numbers represent the row of the queens in the columns from left to right. The first number in the string represents the row of the queen in the leftmost column. The second number represents the row of the queen in the second left column and so on. The example shows a partial solution with seven queens. The missing queen in the seventh column is represented by zero. As a fitness function, we can, for example, use the number of non-attacking pairs of queens. This means, the higher the value of the fitness function, the better the configuration of the queens on the chessboard. Using this fitness function, a solution has value 28. The algorithm starts with an initial population, which it initializes with some randomly generated individuals as it is often the case in genetic algorithms. In this example, it constructs four strings, representing four possible distributions of eight queens on the board by randomly selecting numbers from one to eight. You can see that for example in the blue individual. Three queens are placed in the fourth row. Flying the fitness function leads to the values shown on the slide. You can see that the red and green individuals have quite high values, with 24 and 23. The blue individual has a value of 20, and the individual at the bottom has a value of 11. The population is ranked by the fitness value and the three top individuals are chosen for mating. Green is selected for crossover with blue and green. Crossover of the two pairs of parents leads to four new offspring individuals. For both pairs, the algorithm decide where to cut the two parents to form new children. The first pair is cut after the third position, and the second pair is cut after the fifth position. We could also imagine other selection strategies with other cuts. Now the algorithm recombines the two partial strings to generate four new individuals. Finally, it applies mutations by modifying a single number. Here we see the crossover operation again. Adding a string of numbers is equivalent to cutting through the two chessboards and then gluing the different parts together again. This slide shows the pseudocode of the genetic algorithm. It differs from the previous example in the crossover version, where only one offspring is generated, and the remaining parts of the genes are discarded. Furthermore, individuals are randomly selected for evolution, not based on a ranking. Genetic algorithms have also successfully been applied in aiding the training of neural networks and are used in recent research to generate new neural network architectures. This slides gives references and links to relevant literature if you want to look more into this. Let us look at an example application of a genetic algorithm. This video shows how a little figure is adapted by a genetic algorithm to learn to jump over a ball. The fitness function in this case gives feedback to the construction of the figure. The genetic algorithm favors individuals with a shape that increases their ability to jump higher and wider. Genetic algorithms will return the best fitting solutions after some time. This can also lead to partial solutions, which may or may be not useful to solve an application problem as the outcome of the evolutionary search process is uncertain. Genetic algorithms are incomplete and not optimal, they may or may not find a number of well adapted individuals. Usually, a time limit is defined until which the algorithm runs. If no time limit is set, a genetic algorithm may run forever as it can happen that no individual is found whose fitness value is better than a predefined threshold. Space complexity can be kept constant when the size of the population is kept at a constant number of individuals. This slides summarizes the properties of the local and stochastic search algorithms that we discussed in this lecture. Finally, I want to talk about a class of algorithms that do not use a single agent executing the search, but they adopt a multi-agent approach and try to implement swarm intelligence. A well-known and successful variant of swarm intelligence algorithms are so-called ant-colony optimization algorithms. Ant-colony algorithms are based on observations of ants populations. When ants search for food, they travel in different directions, and they deposit pheromone on the ground in order to mark a favorable path that other members of the ant colony should follow. So, an ant-colony optimization algorithm implements a number of artificial ants and each of these ants builds a solution to an optimization problem. The ants can exchange information on the quality of their solutions via some communication scheme, which is inspired by the deposit of pheromone that real ants use. ACO algorithms are for example applied to traveling salesperson problems. In a traveling salesperson problem, we are looking for routes in a given network of cities, which are connected by streets. An ant is searching for a good tour to visit all cities. In each city, the ant chooses the next city it visits according to some rules. Firstly, since each city must be visited exactly once, in each step the ant decides to move to one of the cities it has not visited yet. Further, the ant is less likely to choose cities that are further away, and the ant is more likely to follow paths with a strong pheromone trail. If the ant has completed its journey and found a short path, it will deposit more pheromones along this path helping other ants to reuse this path. After each iteration of the algorithm, trails of pheromone evaporate. If no ants have traveled a certain path for a while, the pheromone will gradually disappear. This makes it more unlikely that the path is visited again. One big advantage of such a swarm intelligence base's approach is that it automatically adapts if the network layout changes dynamically, because some ants may discover newly open connections and the pheromone on broken paths will evaporate quickly. This slide shows the pseudo-code of the first algorithm introducing the idea of ant colony optimization. So, what have we learned today? We've seen that in very large search spaces heuristic search will fail, because it cannot make sufficiently informed choices in these spaces. For example, very large search spaces exhibit plateaus, which are large regions where the heuristic evaluation of all nodes is identical. Using heuristic search, an algorithm potentially wanders on such a plateau for an infinite time. Local and stochastic search methods give up completeness and optimality, but often produce excellent results in practice. Under certain conditions, we can prove that some of these algorithms converge to the optimal solution with a high probability. We looked at several algorithms such as hillclimbing, simulated annealing, genetic algorithms, and in particular the UCT algorithm, which is one of the most successful stochastic search algorithms. UCT has led to a big breakthrough by AI in game playing. In stochastic search, it is important to find a good balance between exploration and exploitation. Hillclimbing does this in a simple form using random restarts, but we've also seen that UCT does this in a very clever way. It is so successful, because it finds a good balance between exploration and exploitation using the UCB1 values to make and informed selections of nodes. Here again are working questions to repeat the material in this lecture.
