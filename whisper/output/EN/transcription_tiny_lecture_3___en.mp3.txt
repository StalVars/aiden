  Hello and welcome to the lecture on Systematic, Uninformed Search in the Artificial Intelligence Course. In this lecture, we will first introduce some basic terminology and important concepts to talk about search problems. We then look at how we can model a search problem. Finally, we will, after talking about the differences between tree and graph search, introduce several uninformed search strategies. Breadth first search, depth first search, depth limited search, iterative deepening, and finally uniform cost search. The recommended reading is essentially chapter 3 of our AIMA textbook. We list here all the subchapters which cover the material discussed in this lecture. Let us start by introducing the basic terminology and concepts we need to talk about search problems. In the last lecture, we introduced intelligent agents. Let us briefly recall important aspects of a goal-based agent. Through sensors, this agent can perceive the world being in different states and derive a state description. The initial state is the current state of the world. A goal, or several goals, are states that the agent wants the world to be in the future. These states are desirable for the agent. The agent has background knowledge about how the world can evolve, what actions the agent has at its disposal to change the state of the world, and the agent can reason about how its actions change the state of the world. This means that if the agent sets some goals, it can check which actions it should select and execute in the current and possibly subsequent states of the world to achieve a state of the world where the goals are true. This leads us directly to discrete state-based search problems. These problems can be described by a finite number of states and actions. We consider only single agent problems, so we do not consider changes on the world caused by the actions of other agents. We also assume that the world is static, so it is not changing while the agent is deliberating about which actions it should select to reach a desirable goal state. In this course, we only consider the single agent's static case. This is different to games for example where we always need to consider the moves and actions of other agents, our opponents. We will deal with state spaces of games later in the lecture. Further, we assume that the world is observable, which means that the agent has access to relevant knowledge about the world. The agent needs to decide which actions to select, and each action that the agent can execute is deterministic. It leads to exactly one successor state. In other words, given a state of the world in an action that the agent executes, the state reached after the action was executed, is uniquely determined. We now give a formal definition of a state space. A state space is described by a four-tuple consisting of a finite set of states s, a finite set of possible actions a, a transition relation t, and the initial state of the world i. The transition relation t describes what the actions do. So, given a state s in an action a, t defines to which successor state s, the action will lead. We also require that t is deterministic, which means that there is at most one successor state for each state action pair. If a triple s, a, s exists in t, then we say that the action a is applicable to the state s. We say that theta has the transition s, a, s. If this transition is contained in the transition relation t, we can write down transitions by an s for the starting state with an arrow to the successor state s, and on top of the arrow we put the action a. If we don't care about the action that leads to a successor state, we simply draw an arrow from the state s to one of its possible successor states s. Finally, note that the state space forms a directed graph in which the nodes are states and the links between nodes are actions. Let us now look at an example of a state space in the vacuum cleaner world which we have already met in the last lecture. Recall that a vacuum cleaner agent is in a world consisting of two rooms. Each room either contains dirt or it does not contain dirt. Each of the orange rectangles represents a possible state of the vacuum cleaner world. In the initial state, which is marked in red, the vacuum cleaning agent is in the left room and dirt is only in the right room. In each state, the vacuum cleaner agent can either suck, here denoted by s, move left, here denoted by l, or move right, here denoted by r these are all the possible actions in the vacuum cleaner world. Note that in some state, an action might not do anything. For example, if the agent moves left or sucks in the initial state, it simply stays in the same state. The transition relation is described through directed edges leading from one state to another or the same state. Now we define a search problem. A search problem is described through a 6-tuple s, a, c, t, i, sg, where s, a, t and i are as they were defined in the definition of a search space. Additionally, we have a cost function c, which gives for each state an action pair, s, a, the cost of applying action in state s. Usually, we consider only non-negative costs which range between 0 and some positive number. Further, the set s, g, which is a subset of the set of states s, is the set of goal states of our search problem. We say that our problem has unit costs if the costs of applying any action in any state is exactly 1. Let us return to the example of the vacuum cleaner world. Each directed edge representing an action has now a cost assigned to it shown by the green number written next to the edge. Note that in this case all actions applied in any state have cost exactly 1, so the vacuum cleaner world presented here has unit costs. The two goal states where both rooms are free from dirt, are marked in blue. We now introduce some more terminology on the reachability of states. We say that a state s, is a successor of a state s if an action exists which takes the world from state s to state s. We then also call s the predecessor of s. A state s, is reachable from a state s if there exists a sequence of transitions leading from s to s. This means that we can find a sequence of actions such that by applying the first action a1 in state s, the second action a2 in state s1 and so on, we will reach the desired state s, after having applied some number and of actions. It is possible that n is equal to 0, so the state s is equal to the state s. For example this holds if the initial state is contained in the set of goal states. Then, the goals are immediately satisfied. The sequence of actions to get from s to s, is called the action path and the sequence of states to get from s to s to the state path. So, when we speak of a path, we can describe it either by the sequence of states through which we go from some state s to some state s, or we can describe it by the sequence of actions that we apply to get from some state s to some state s. A path is uniquely determined by either the sequence of states or the sequence of actions because our transition relation is deterministic. The cost of a path is the sum of the costs of all the actions along the path. If we simply say a state s, is reachable without giving an explicit reference state, we mean that it is reachable from the initial state i. We say that a state s is solvable if there is some goal state s, which is reachable from s, otherwise we call s a dead end. So, for any state s, s is either a solvable state from which some goal is reachable, or it is a dead end. This is also so called trap state, from which no goal state can be reached anymore. Let us look at our vacuum cleaner world example again. We are given the initial state on the left and the states and actions are as before. Are all states reachable? In other words, can we find a path to any state in this graph, starting from the initial state? In this example we can clearly say no, not all states are reachable. Look at any one of the orange states for example. We cannot find a path taking us from eye to any one of these states, since dirt cannot suddenly appear in this world, because we do not consider possible actions of other agents in this model. Furthermore, the vacuum cleaner has no action available to create dirt. The green states as well as the initial state are, however, all reachable. Recall that the two states with no dirt in either room are goal states. Are all states solvable? Yes, if you look closely, you can see that we can find a path that takes us to one of the goal states from every one of the states. This means that we have no dead end states in this example. We will look at some examples with dead ends later. Conversely, we see however that the agent cannot always go back to an earlier visited state. If we assume that it starts in a world with two dirty rooms, it can move in this world. But once it sucks, it moves closer to the goal where we want to have both rooms cleaned. So, if the agent had some way to create dirt, or if we had another agent creating dirt, then of course we would have very different transitions. But as we said in the beginning, we consider single agent problems and the world is static. It does not change, so once the vacuum cleaner has removed dirt, then the room will stay clean. Let us now talk about solutions to search problems. Let us assume that we are given a search problem description pi as the six tuple, and let us be some state in our state set. A solution for S is an action path from the initial state i to S. If the state S is a goal state, then the solution for S is a solution for theta and denoted by rho. A solution for any of the goal states is called a solution for the problem theta. The set of all solutions for theta is denoted by S theta. If at least one solution for theta exists, then we say that theta is solvable, otherwise it's unsolvable. We might find more than one action path reaching a goal state and all these solutions may have different costs. A solution is optimal if its cost is minimal among all solutions for S. We denote the cost of the optimal solution of a state S as C, S. Whenever you see the star notation, this means that we are talking about minimal costs for minimization problems or maximal costs for maximization problems. In any case, we always mark the optimal costs with a star. Going back to the vacuum cleaner agent example again, we see that the problem is solvable. One possible solution is described by the green solution path leading from I to the goal state on the bottom right. Can you see other solutions? Another solution would for example be if the agent first tries to move left, so it remains in the initial state before taking the green solution path. This would be a different solution with cost one unit higher than the green solution path, because the agent fails in moving left staying in the same room and then moves to the right room, which is dirty. There are many more possible solutions, take it as an exercise to see if you can find any others. Since a solution exists, the vacuum cleaner world is a solvable search problem. You can see that in this graph we have loops, where the agent can execute actions which also cost one unit, but these actions will not take the agent closer to the goal. In this example, such a behavior would not be harmful, but it will consume or accumulate costs, for example by consuming energy and therefore lead to non-optimal solutions. Now that we have introduced all the necessary basic terminology and concepts, we can start talking about how to model a search problem. During search, we build up a so-called search tree. In the root node of the search tree, we have the initial state. In each state, several possible actions can be taken next, which we can represent as child nodes of the node representing the original state. This results in a representation of the search as a tree. We speak of a search node, which contains the state it represents plus some information on how it was reached. Then, we need the concept of node expansion. Being in a node N, we need to generate all the successors of the node by applying all actions, which are applicable to the node state S. So, we take the state information that we have in this node, we check which actions are applicable in this state, and for all the applicable actions A, we generate a child node for each of their successor states. Once node expansion has been applied to a node N with state S, we say that the node N has been expanded. You can probably imagine that we can expand nodes in very different orders. The search strategy is a method for deciding which node we want to expand next. A search strategy takes two important lists as a basis for further decisions, the frontier and the explored set. The frontier, also called open list, is the set of all nodes that are currently candidates for expansion. The explored set, also called closed list, is the set of all states that have already been expanded or visited. Finally, if there are loops in the state space, it may happen that during search we visit a state meaning that we generate this state for expansion, which we have already visited before and thus this state is already in the explored set. We call these states duplicate states. Note that the nodes of duplicate states may never contribute to an optimal solution. Let us now look at the search tree of the vacuum cleaner world. In the root node of the search tree, we have the node representing the initial state. By expanding this node, we obtain three child nodes, because three actions can be executed in the initial state. The vacuum cleaner can stay in the same room and suck, or it can move left or right. Sucking and moving left lets the vacuum cleaner stay in the initial state as both actions have no effect in the initial state. The nodes generated in the search tree contain duplicate or repeated states. After one expansion step, the all child nodes of the root node are candidates for expansion. The frontier is hence equal to the set of these three child nodes. The root node is the only node which has already been expanded and hence is part of the explored set. Let us now look at a different example, the Romania travel example. In this problem, the agent looks for a root from a rad to bookerist. From this slide, you see a map showing several Romanian cities and the travel times between these cities. The state space of this problem is described through a graph. The states are the different cities, depicted by nodes, and actions are trips between the cities, depicted by edges. Note that the edges have no directions here, so actions are possible in both directions. Action costs are given as distance information on the edges. You can see immediately that this problem does not have unit costs, but all the costs are positive, which is very natural in this case as the distances to travel between the cities are of course positive. We have an initial state which is the city of a rad, and there is a goal state which is the city of bookerist. Our agent is looking for a solution that gives a path from a rad to bookerist in which minimizes the distance to travel. An optimal solution for this search problem would be to find a path from the initial state a rad to the goal state bookerist with shortest path costs. Let's now look at how the search trees builds up in the Romania travel example. The root node corresponds to the initial state, the city a rad. From this city, we can travel to three neighboring cities. We can go to Sibiu, Timisora, or Zerrand. After we've expanded a rad and added its child nodes, we are at the tree shown in the middle of the slide. The search tree results from a sequential description of the visiting order. It is very important to understand the difference between the state space graph and the search tree. Usually, state spaces are graphs. Rarely, it may happen that a state space is a tree if there are very limited ways of executing actions. Usually, we are searching a state space which is an arbitrary graph. As we search the state space graph, we build a search tree over the state space graph. The search tree is a description of the sequential visiting order. Note that while we may encounter duplicate states in the search tree, these states will belong to two different nodes, since they have been reached along different paths. The search tree thus does not contain any loops whereas the state space graph can. Let's assume our agent decides to go to Sibiu. From Sibiu, it has four possible options. It can go back to a rad or it can visit one of the three neighboring cities of Sibiu, Fagueras, Oradia and Remyku Viltra. After Sibiu has been expanded, we are at the tree shown on the bottom. A rad is a repeated state. We have already been in a rad, but of course, nothing prevents us from going back. The slides also shows the frontier of the search after two expansions. The frontier of this search tree are all the nodes that are candidates for expansion. The agent still has two options to where it can travel from a rad, and it has three more cities that it can travel to from Sibiu. We have just seen that after two expansions we have a repeated state, a rad. Repeated states are created by loopy paths in the state space graph, and, as we have already discussed, loopy path can never contribute to an optimal solution. This is because in search problems with positive action costs, any loop accumulates costs and visiting states repeatedly does not take the agent closer to the goal or open up new action possibilities as we consider a static world. It's imagined that we have two possible or redundant paths from one city to another as shown based on an extract of the Romaniah map. We have two possibilities to reach Bucharest once we are in Sibiu. We can either travel via Fagueras in the upper part of the graph, or via Riminiku Viltra and Ptesti at the bottom. If we look at the length of these trips then we see that the cost of the trip via Fagueras is 310 units, and the one via the other two cities is in fact shorter with cost of only 278 units. This means that even once we have found a solution, we cannot be sure that it is the optimal one. Let us look at one more example, the so-called 8 puzzle. We have a grid with 9 cells, 8 of these cells are covered by tiles, and 1 cell is empty. This empty cell allows us to move the tiles. For example, if you look at the start or initial state, we see that we could move tile 5 to the right, or we could move tile 6 to the left, or we could move tile 2 down, or tile 3 up. This tile can only be moved in a horizontal way or in a vertical way if a neighboring cell is free. On the right side, we see the goal state. We want to sort all the tiles by their numbers, and we want the empty cell to be in the upper left corner. So what is the set of possible states? I give you a few hints, and I also invite you to fill out this chart by applying the definitions that we introduced on the previous slides. The sets of states is the set of all possible configurations of the tiles. This is one possible description. Another possible description is to look at the position of the empty cell, but you also need to record in the state description where the other numbers are placed. Think about how you can do this in an efficient way. The initial state is the specific arrangement of the tiles that we see on the left hand side. For the set of actions, you again have several choices of how to describe it. For example, you can define actions for each possible tile, so you can in principle move the tile number seven up or down or left or right. Of course, in certain states many of these actions will not be applicable. If you describe the actions like this, you will always have to go through a large set of possible actions and check for the ones that are applicable. A more compact representation is to simply consider the moves of the empty cell and by moving this empty cell, the other tiles also move. For example, in the start state if we say we move the empty cell up, then tile two moves downwards. So moving the empty cell up is one possible vertical action that we can execute in the initial state. We can also move the empty cell down, which means that tile three moves up. Again, we have exactly four applicable actions in the initial state. The goal state can be described by the position of the empty cell and the tiles. Finally, for the path costs we can think about the cost that each move has. In this example, it seems very natural to use unit costs because there is no difference in the effort for executing the different actions. So, when we assume the description where we have the empty cell moving, then this action always costs this one unit, independently of the direction. Additional notes. States. Location of each tile and the blank initial state. Initial configuration of the puzzle actions. Move the blank left, right, up, or down goal states. The state that matches the goal configuration path costs. Each action costs one unit. Let us look at one more example. You see here a description of a state space as a graph. Each node in this graph is considered a state. On the left, you see the initial state i as a dot with white filling, and on the right, you see three goal states represented as dots with light blue filling. Between these we have other states shown as dark blue dots. Between all states, we have directed edges describing the possible transitions. Each edge is marked with an action name and the cost of the action in braces. In the initial state, we can apply action a, which has cost 2, and leads us to successor state s2. We can apply action c, which has cost 3, and leads us to state s2. We can apply action b with cost 4, which leads us to state s3. By looking at the action costs, you clearly see that this problem does not have unit costs. For each state we can determine the set of applicable actions by looking at the outgoing edges from the state. In the case of the initial state, we could apply actions a, b, or c if you look at state s6, which is in the middle at the bottom of the graph, you see that only one action e is applicable, which has cost 0, and takes us to state s3. From state s3, we can then apply the same action, which takes us back to s6. Do we have a deterministic transition relation in this problem? So, we don't look for example at action g in one of the goal states sg1. You can see that action g can lead us either to goal state sg2 or to goal state sg3. So, applying action g in the state sg1 can take us to two possible successor states, which means that the transition relation is not deterministic. Let us look at the reachable and solvable states in this example. Are all states reachable? Can we find an action path starting from the initial state and leading to any state in this graph? In this example, this is clearly not the case. Not all states are reachable. Look for example at state s7 at the bottom right of the graph. State s7 has only outgoing edges, so once we are in state s7, we can apply action f or action h, but the state s7 can never be reached, because it has no incoming edges that lead to this state. Are all states solvable? Can we reach some goal from any state? No, clearly not. Look for example at state s2 in the upper left half of the graph. State s2 is a dead end, it has no outgoing edges. If we decide to apply action c in the initial state, we end up in state s2. From there we can never leave again. What are the optimal solutions to this problem? Note that actions e and g both have cost zero and thus applying them does not increase the cost of the path. For an optimal solution in this state space, we start at the initial state i, we apply action b which leads us to state s3. From there we can loop through the cycle between s3 and s6 by applying action e as often as we like, because it doesn't add any costs to our solution. If you look at the graph, you can see that the only action applicable in state s6 takes us back to state s3. So, we go back to s3, there we apply action c which has cost 3. Then we again apply action e with cost zero and this takes us to goal state sg1. In this state, we can apply action g as many times as we want, because by applying g we only move between the goal states and g has cost zero. However, once we've ended up in state sg2, which can happen because the transition system is non-deterministic, we will be in this goal state and cannot apply any more actions. The cost of this path is 7. I invite you to take this example as an exercise. Look at other potential solutions that lead you to a goal state and see what costs they have. How can we describe a search problem through an implementation? We essentially construct an API that will help us access the states from state space through the nodes and simulate the execution of actions. A possible data structure of a search node holds the state representation in a compact form. We also want to keep information about where we came from. Although each node stores information on its parent node, which is its predecessor node in the search tree, the action that was taken from the parent node to get to the current node in the cost of the path taken to get to the current node. We assume that we are looking for a path from the initial state to the goal state. By executing actions, we will have reached this node in the graph and we have accumulated a certain amount of costs because we've taken a specific path through our state space. We also need several methods to implement operations on the state space. Given a problem representation, we need to access the node representing the initial state. There needs to be a goal test method because whenever we reach a node and we look at its state description, we want to test if we have reached a goal or if we need to further explore the state space. This goal test returns a Boolean, and it returns true if and only if we are in a goal state. We also need to determine the costs of actions, which is done by the cost method. The actions method returns the set of actions that are applicable in the current state. Using the child state method we compute the child or successor state that results if we apply one of the available actions to a state. This method implements the transition relation. Once we have reached a node, we want to be able to extract the solution path if the search has taken us to a node with a goal state. We also need to be able to expand a node and generate its child nodes. Let's assume we decide to apply a certain action in a state of a node. Then we need a method that computes the transition from the state to the successor state and it also constructs a new node in the graph. The new node has a state the successor state, as parent the starting node, node, as action the action, action, that has taken us from the parent node to this node and as path cost the path cost of node, plus the action cost of the action we have taken. Furthermore, we have operations on the frontier. At each point we need to decide which node to expand next. All the nodes, which are candidates for expansion, are kept in the frontier of our search. Once the frontier is empty, we have no nodes and states left for exploration and search terminates. We will look at this in more detail later. We also need to regularly add or remove nodes too and from the frontier using the pop and insert functions. The pop takes the first node of the frontier and removes it from the list. Insert through adds new nodes to the frontier. We will later see how search algorithms differ from each other when inserting nodes in the frontier. We have three possible ways of describing a search problem. We can first develop a so-called black box description. This essentially means to construct an API, like the one we have just looked at, which will help us to access the state space and simulate the execution of actions. The state space is only indirectly accessible by working with the methods provided in the API. Furthermore, we can develop a white box description. A white box description is an accessible, but compact representation of states, actions, and goal tests. For example, we can be given a map of a certain area showing the state space implicitly, and we look for a path on this map to travel from one state to another. Finally, we can have an explicit description of the state space, which means to represent all the states of a problem in an explicit state space graph. To describe a search problem with a black box description, we basically give an API to construct the search space. We give the initial state of the problem, a goal test method which takes as input a state and returns true if and only if the state is a goal state. Further, we give a cost method which returns for every action the cost of applying this action, an actions method which returns for every state the set of applicable actions in this state and finally a child state method which returns for a state and action the successor state that is reached when applying the action in the input state. In the white box description, an accessible yet compact description of the state space, states, actions, action costs and goal states are given for example in the form of a graph. We have already seen such a white box description in the Romania travel example. Let's look at another white box description, this time for the vacuum cleaner agent. Our little vacuum cleaner lives in a very simple world with only has two possible rooms and in these rooms there is either dirt or no dirt. This leads to eight possible world states shown on this slide. The vacuum cleaner agent has three possible actions in each state. It can either move left or right or it can suck. Its goal is to have no dirt in the rooms. Its path costs can be computed by summing up the action costs. In this example, we assume one cost unit per action. Finally, we look at the third way of presenting a search problem, explicit descriptions. An explicit description of the vacuum cleaner agent can be seen here. The eight possible states of the state space are linked with all the possible transitions between the states. For this simple variant of the problem, we can easily fit this on one slide. But when adding for example more rooms, the size of such a representation would quickly explode. Now that we have seen how we can model search problems, we actually start looking at some systematic search strategies. We now introduce two important concepts, tree search and graph search. In tree search, we assume that the search space graph is a tree. In graph search, the search space graph may contain loops and different paths leading to the same node can exist. When we perform tree search, we do not need to remember visited states, because if the search space has a tree structure, each state can only be visited via exactly one path from the initial state. Consequently, when we do tree search in an arbitrary graph, we will not know whether we have generated repeated states. In graph search, we remember visited states. We keep an explored set into which we put all the states that we have already explored, and we use a technique called duplicate elimination, which means that if a generated state is in the explored set, then we skip it, otherwise we explore it. Recall the vacuum cleaner example. You saw in this example that the agent could execute some actions, which take it back to the same state. This means that it remains in a state, which is already in the explored set. By doing this repeatedly, an agent will accumulate a lot of costs, but never get any closer to the goal. By keeping a set of explored states when searching a state space graph, an agent knows which states it has visited before. On this slide, we compare the tree search and graph search algorithms with each other. On the top you see tree search, on the bottom you see graph search. The basis of the tree search algorithm is as follows. In each step, a node is picked and removed from the frontier. If this node is a goal state, then the algorithm returns the corresponding solution. Otherwise it expands the chosen node and adds the resulting child nodes to the frontier. When we do graph search, we add three important lines to the algorithm, which are marked here in red. First, we initialize our explored set, which is initially empty. Then, we choose a node from the frontier, remove it from the frontier, but this time we also add the node to the explored set. When we expand this node, we do an explicit test to check whether the child nodes are already in the frontier or explored set or not, and we only add new nodes to the frontier that are neither already in the frontier nor in the explored set. We now summarize important criteria for evaluating search algorithms. First of all, we require a search algorithm to be sound. Whenever the search algorithm returns an action path, this path is a solution to the search problem. Second, we are interested in whether a search algorithm can find a solution if one exists. If the answer is yes, we call the search algorithm complete. We also want to know if a search algorithm is optimal, meaning it guarantees to find solutions of minimal costs. We are also interested in assessing the complexity of a search algorithm. How much time will it take to search for a solution, and how much memory will it require? Time complexity is measured in the number of states and algorithm has to generate until it finds a solution. Space complexity is measured in the number of states that it needs to remember. There are two very important features that influence the complexity of a search algorithm. The first one is the branching factor. In general, we have several actions that are applicable in a state. The branching factor of a node is the number of actions that are applicable in this state. So, if many actions are applicable in a state, then the branching factor is very high. For the vacuum cleaner example, the branching factor of every state is 3, because in each state the number of actions that the vacuum cleaner can execute is 3. For other examples, such as the Romania travel example, we have different branching factors for different nodes. We can consider the worst case, so the highest branching factor of a state, or look at the average branching factor. The goal depth is the depth of the search to which we need to search to reach a goal state. For some domains, we can precisely estimate the length of a solution and how deep we need to search. Let's now look at various search strategies. We start with four search strategies that use no information about the length of the search path or about the cost of this path. Bread first search, depth first search, depth limited search, and iterative deepening search. Then we also look at one search strategy that takes into consideration the current path costs. This strategy is called a uniform cost search. Let us first look at bread first search. In this search strategy, nodes are expanded in the order they are produced. The frontier is a FIFO, first and first out, Q. Let's discuss the small example on this slide. Note that the white nodes are generated nodes, and the gray nodes are explored nodes. Let's assume bread first search is in some node A from node A. It can expand two potential other nodes B and C. It expands node A, and thus adds nodes B and C to the frontier. The strategy now visits node B, and expands BB also has two successor nodes and these nodes D and E are added to the frontier by appending them at the end of the queue. We now continue to visit node C C has successor nodes F and G. We again expand node C and add nodes F and G at the end of our frontier queue. The frontier queue now contains nodes D, E, F, G, and bread first search continues to visit the nodes in this order. It visits node D, it has no successor's, and thus there is nothing to add to the frontier. Then it visits node E, again no successor's, and so on, until it has explored all the nodes. This slide shows the algorithm of bread first search. Here you see underlined in red the duplicate check against the explored set in the frontier, and the goal test before adding a node to the frontier. We are conducting graph search and not research. The frontier is initialized as a FIFOQ and the pop operation always returns the shallowest node from this FIFOQ. The goal test is performed at node generation time. In this strategy, we know this is the shortest path in the number of steps taken towards the goal state, and thus can just stop. Note that if actions have different costs, the found solution will be shortest in number of actions taken, but may not be minimal in terms of path costs. Let us look at an example of the bread first search algorithm working on the Romania travel example. The algorithm starts at the city Sibiu. The frontier is initialized with Sibiu as the only element in it. The next step expands Sibiu. During the expansion, Riniq Vilcha and Figeras are both added to the frontier. The next node to be expanded is Riniq Vilcha, since the algorithm always expands the first node in the list first using the pop function. When expanding Riniq Vilcha, it adds Ptesti at the end of the frontier. Sibiu is not added to the frontier again, since it is already in the explored set. Figeras is now the next city to expand next. Bucharest is the only unexplored child of Figeras. Running the goal test on Bucharest returns true, so the algorithm returns the solution, which is the path from Sibiu to Bucharest Vifigeras. Note that this is the shortest path in numbers of actions taken, but not the shortest path in distance to the goal. The example confirms that bread first search always returns the shortest path in numbers of actions, which may not be the shortest path in terms of path costs when action costs are different. Let's look at the properties of bread first search. This algorithm finds the shallowest goal state first, because as you can intuitively imagine from the graphical representation, it searches a state space in layers, so it will find the goal state on the shallowest layer first. Completness is obvious when we have finite search spaces. If we, however, have a non-finite action space, it would be incomplete as the maximal branching factor is infinite. We do not consider non-finite action spaces. The solution is optimal provided every action has identical and non-negative costs, so this algorithm works very well for unit cost problems. In the Romania travel example, we have seen that actions have different and non-unit costs. This means that a solution found using the bread first search strategy can be suboptimal. We now summarize the time and space complexities of bread first search. Let b be the maximum branching factor and let d be the depth of a solution path. Bread first search has exponential time and space complexity. The maximum number of nodes that is expanded is of order b to the power of d, so in the first layer we expand up to b nodes because our branching factor is b. Then in the second layer we expand up to b children of each node, so it's b to the power of 2, and in the third layer we have b to the power of 3, and so on. Space complexity of the strategy is exponential as well because every node that we generate is kept in memory. It's in fact the sum from n equals 0 to d over b to the power of n, so the space needed for the frontier is of order b to the power of d. The space needed for the explored set comprises all nodes on all previous layers, so all layers of depth up to and including d minus 1. Now let's look at depth first search. Since we saw that bread first search has a very high memory consumption, we are interested in a search strategy that needs a lot less memory. This is where depth first search comes into play. In depth first search, we always expand the most recent node in the frontier first. Our frontier is organized as a leafo queue. Using a leafo queue means that we insert nodes at the beginning of the queue. When a node has no children, search backs up to the next deepest node that has unexplored children under this strategy. Let's look at some illustration again. Assume depth first search is in some node a node a has two successor nodes b and c, the search strategy adds b and c at the beginning of the frontier and continues to work on bb as children d and e they are again inserted at the beginning of the frontier queue before c. This forces the strategy to explore d next. From d it explore h and so on until it reaches a state with no children. Then it starts working on the next node in the frontier, which is the next deepest node that has unexplored children. Here is a larger example of depth first search. We again assume that we have some nodes without any children and that m is our only goal node. What you see in which order the strategy explores nodes in the search space. The search builds up the search tree until it finally reaches the node m on which the goal tests exceeds. This is the pseudocode of the department's first search algorithm. As you can see, we present here again a graph search algorithm. Depth first search as tree search works similarly, but is not using the explored set. This algorithm could also be implemented as recursion where we recursively expand nodes to obtain their children and do this until we reach a goal state our frontier becomes empty. We now summarize the properties of depth first search. In general, a solution found is not necessarily optimal. Depth first search is also incomplete in the general case. For example in an infinite state space it can happen that it descends on an infinite path forever without finding a solution. Completness can be guaranteed if the search space is finite and graph search is used, so the algorithm remembers the visited nodes. In infinite state spaces, the algorithm might as I said before descend forever on infinite paths. Tree search may loop forever in repeated states even in finite search spaces because it does not remember which states it has visited. Let us discuss the time and space complexity of DFS. The time complexity is exponential and similar to breadth first search, but with exponent m, the maximum depth of the search space, instead of the depth of the solution, because depth first search will search until it reaches in the worst case the maximum depth of the search space. So, instead of finding shallow solutions, it may find very deep solutions first. In the worst case, all nodes are visited before a solution is found. This can be even larger than the state space if tree search is performed on graphs where the algorithm does not remember already visited states, because it might expand a state several times. The space complexity on the other hand is linear and of order b times m or m only. It is thus much smaller than for breadth first search. Depending on how the depth first search algorithm is implemented, it needs different amounts of memory. In the pseudocode, you can see that we need to store the nodes along the current path that we expand in all their neighbors that have not been visited yet, so all nodes that are candidates for expansion. If we implement depth first search using clever indexing, we only need to store the nodes along the current path and compute their unexplored neighbors dynamically in an efficient way. This reduces the space complexity to m. Our next search strategy is depth limited search. This is simply depth first search with an imposed cutoff on the maximum depth of a path. For example, in root planning, if we have n cities, we know that the depth of an optimal solution is at most n1. It is exactly n1 if we need to visit all the n cities to get to the goal city, because we need one action between each city pair along the path, so we need to apply n1 actions. In the Romania travel example, a depth of only 9 is sufficient, since every city can be reached in at most 9 steps. Take a look at the graph and explore why the depth 9 is sufficient for this graph. This parameter can be determined using general graph analysis algorithms, which are able to determine graph parameters such as the depth and width of a graph. Let us look at a recursive algorithm for depth limited search performing tree search. The limit counter is marked by a red arrow. In each recursive expansion, the algorithm counts down the predefined limit. It continues with a neighboring node when reaching limit 0 and it has not found a solution by then. It may also stop earlier if the goal test has succeeded. If the depth limit is set to a two small value, then this search algorithm will be incomplete, and it will not be able to find a solution. Before we summarize the properties and complexity of depth limited search, it is complete only if the depth limit is not smaller than the length of the shortest solution. The first solution found may not be optimal. Time complexity is exponential and space complexity linear as with depth first search, but with the parameter M now replaced by the depth limit L. iterative deepening search takes the idea of depth limited search, but in every iteration, it increases the search depth by one. It thus performs several rounds of depth limited search with increasing depth in each round until infinity or depth limit is hit. Let us look at an illustration of the iterative deepening search algorithm. It starts with limit 0, which means it simply generates the node A, but does not do expansions because the depth limit of 0 is reached. This completes the first round. Now we set the depth limit to 1. The algorithm starts again at node A, but this time it also expands the node a generating nodes B and C as it does not find a solution within this depth. It repeats searching from the root A again now with the limit set to 2, and so on until a solution is found or a finite search space is exhausted. On this slide, you see the iterative deepening search strategy illustrated for limit 3. And now you also see clearly that this algorithm does in fact perform a lot of repeated work. The question is, does this make sense? It turns out that yes, it does, because the iterative deepening algorithm combines the advantages of depth first search and breadth first search. The algorithm is optimal for unit action costs, because repeatedly increasing the limit of the search depth halos it to find solutions of minimal length first. Note that there are also extensions of this algorithm that can guarantee optimality for general action costs. The algorithm is complete for search spaces with finite branching factors because the search depth is limited by the depth limit, and the finite branching ensures a limited width of the search tree. Time and space complexity are the same as for depth limited search. Let us compare the time complexity of breadth first search and iterative deepening search. Breadth first search searches through the search space layer by layer. It first expands all successors of a node at level 1, where there can be up to B successors, then it expands all successors of successors and so on. It has thus time complexity of order B to the power of D. Iterative deepening search does something very similar, but it repeats these expansions for each new depth limit. The initial node will in fact be expanded D times, the next layer, so the children of the initial state, D1 times, and so on. However, the majority of the work is done in the last layer, which is of size B to the power of D and where the algorithm encounters most of the nodes. So even if a lot of the work is repeated, it does not influence the time complexity the algorithm requires. So as in breadth first search in the worst case, the algorithm might generate of order B to the power of D nodes. However, iterative deepening helps us to control space complexity and allows us to search spaces where we don't know the depth at which we will find a solution. Additionally, optimality for unit action costs is a very attractive property of this algorithm. Let's look at an example of time complexity when the branching factor V is 10, and we search up to a depth of 5 until we find a solution. In the table, you see the number of nodes that are generated by these two strategies at each layer. breadth first search will generate over 111.000 nodes. iterative deepening, even though it repeats all the work on previous layers several times, will only expand 123.000 nodes, which is just a little more and makes this algorithm very attractive. Finally, let's look at uniform cost search. When an algorithm has built up a partial solution, which means it has constructed a path from the initial state to some node n, then it knows the path costs at this node n. The algorithm can use this information to organize the frontier as a priority queue. We have already seen two organization principles for the frontier. We have seen that we can organize it as a FIFOQ or as a leafOQ. Now, the nodes are ordered by path costs. Uniform cost search expands the path, which has lowest costs. Uniform cost search can find an optimal solution if all actions have non-negative cost. If all action costs are non-negative, the path costs grow monotonically, which means that the path costs of any successor of n must be equal or higher than the path costs of the node n. On this slide, we see the algorithm for uniform cost search. Note the goal test is done at node expansion time rather than at node generation time, which ensures that the algorithm finds the optimal solution. Whenever it pops a node from the frontier, which is organized based on path costs, the algorithm checks if this node is a goal node. If this is the case, it returns the solution. Otherwise, it adds this node to the explored set and expands the node. This algorithm performs graph search as can be seen from the usage of the explored set. The corresponding tree search algorithm would not use an explored set. For each of the child nodes, the algorithm check if its state has already been explored or is already contained in the frontier. If not, this child node is inserted in the frontier, which is sorted after each insertion according to ascending path costs, such that the node with lowest path cost is at the beginning of the list. In the last two lines, the algorithm checks if there is a node with lower path costs in the frontier leading to the same state as the child node that was just inserted. If this node has higher path costs, it is deleted from the frontier and replaced with the child having cheaper path cost. Let's look at an example. We are starting in Sibu and want to get to Bucharest. What happens if we apply uniform cost search? We are in the city Sibu, simply abbreviated with SS has two child nodes F and Rv, which we add to the frontier. We have Rv with path costs 80, and we have F with path costs 99. We pick Rv since it has smaller cost and expand it. From Rv, we can go to the city P, which adds costs 97 to the path. So now, the path from SvRv to P has costs 177. We could also go back to S, but we have visited S already, so we do not consider it further. The frontier now contains F and P. The path to F has cost 99, and the path to P has cost 177. So, the algorithm expands F next and reaches a goal state, Bucharest, on a path of cost 310. B is now added to the frontier. Note that UCS does not perform a goal test on B yet and so the algorithm does not terminate here. It still has P with costs 177 in the frontier. Since its cost is lower than the cost of B, P is expanded. And indeed, from P the algorithm can reach B again with additional cost 101, so in total with path costs 278. Since this path to B is shorter that the path to B with costs 310, which is already contained in the frontier, B in the frontier is replaced by the new node B with lower costs. B is expanded in a goal test made on Bb is a goal state, so the algorithm terminates once the goal test is run, and returns the path VRv and P with total costs of 278 as the solution. On this slide, we summarize the properties and complexity of uniform cost search. It is optimal for non-negative action costs. Whenever a node is selected for expansion, the optimal path to this node has been found. The algorithm does not care about the number of actions on the path, but only about the total path costs. This also means that it will get stuck on infinite paths with zero cost actions where it loops or descends forever. The algorithms is complete if all action costs are larger than zero. Time and space complexity of this algorithm are B to the power of 1 plus the path cost of the optimal solution divided by the minimum action cost. The one occurs in the exponent, because once we have reached a goal state, we still need to expand all remaining unexpanded nodes with shorter path costs to be sure that no cheaper path to the goal can be found. If all action costs are equal, then the complexity is B to the power of D plus 1. Uniform cost search is very similar to Dijkstra's algorithm, and in fact equivalent on the state space graph. If you look at the definition of these two algorithms, you notice only two small differences. A first difference is that uniform cost search generates the graph incrementally whereas Dijkstra's algorithm works on an input of the explicit whole graph, like the explicit state space representation introduced earlier in this lecture. The second difference is that uniform cost search terminates when it has found a minimal cost path to some goal state, rather than the fixed target state that is given in the input to Dijkstra's algorithm. The slide also shows an animation of Dijkstra's algorithm taken from Wikipedia. We now present an overview of all the algorithms that we talked about. You see their time and space complexities, whether they are complete or not, and whether they are optimal or not. If certain properties only hold under certain conditions, we have also added these conditions. The conditions often depend on the branching factor, or on the costs of actions, which must satisfy certain properties. Furthermore, many properties only hold when the search space is size as finite. On this slide, we summarize the main takeaway messages. Depth first search is very attractive because it uses only linear space. Compact encodings for explored sets of exponential size exists, which help to use this algorithm on very large and very complex graph structures. Depth first search is rarely found in practice, but it does not mean that there are no applications where this method is an ideal choice. Remember it is optimal for unit action costs, and will work very well for low branching factors and short solution lengths. Depth limited search prevents that depth first search descends forever on an infinite path. Iterative deepening search is the preferred uninformed search method, when there is a large search space, and we do not know the depth or length of a solution, since it prevents that we descend forever on an infinite path. Uniform cost search is an algorithm that finds optimal solutions for problems with non-unit action costs. On the last slide I have put together some summary questions for you. I recommend you go through the slides again and read the chapters of our textbook and then write down your answer to these questions. The summary questions will also help you to prepare for the exam. You need to know the relevant concepts and terminology that we use to describe search problems. Having understood the difference between tree and graph search knowing what the set of explored nodes is used for, makes you well prepared to also understand the algorithms that we covered here. You should be able to explain the basic principles of how these algorithms work. The key is to understand the difference between breadth first search and depth first search, which is rooted in the organization of the frontier. Depth limited search and iterative deepening search are variants of depth first search. Uniform cost search is an algorithm that organizes the frontier based on path costs. Be able to discuss and compare the algorithms with respect to time complexity, space complexity, optimality and completeness. This takes us to the end of the lecture on uninformed search.
