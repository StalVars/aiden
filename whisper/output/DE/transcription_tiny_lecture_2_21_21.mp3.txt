  Um das Leistungsmaß zu berechnen, müssen wir berücksichtigen, dass jede Aktion der Agentinnen die Welt in einen anderen Zustand bringt. Wenn die Sequenz der Welt zustände für einen externen Beobachter wünschenswert ist, hat die der Agent in gut agiert. Lasst uns annehmen, dass wir auf einem Stuhl sitzen und den Staubsauger Agenten beobachten. Er führt verschiedene Aktionen aus, bewegt sich von einem Raum in den anderen und saugt. Basierend auf unserem Leistungsmaß beobachten wir die Sequenz der Welt zustände und beurteilen, ob der Agent gut oder schlecht agiert hat. Das Leistungsmaß ist unabhängig vom Agenten. Es basiert auf der Wahrnehmung eines externen, objektiven Beobachters, der nur auf die Zustände der Umgebung berücksichtigt. Anhand falls könnte der Agent perfekte Rationalität und maximale Leistung erreichen, indem er sich selbst belügt, dass seine Leistung perfekt war. Es ist wichtig festzuhalten, dass wir das Verhalten bekommen, wofür wir den Agenten belohnen. Ein intuitives Leistungsmaß für unseren Staubsauger ist die Menge des aufgesaugten Schmutzes zu messen, woraus sich mit einiger Zuverlässigkeit auf die Sauberkeit des Raumes schließen lässt. Wenn man jedoch genauer darüber nachdenkt, könnte der Agent leider auch ein ungewolltes Verhalten entwickeln, welches zu einer Maximierung des aufgesaugten Schmutzes führt. Er könnte Schmutz aufsaugen und direkt wieder ausschpucken, sollte er die Fähigkeit dazu haben. Wenn der Staubsauger dies wiederholt, kann er sein Leistungsmaß maximieren ohne den Raum zu säubern. Ein aggressiverer Staubsauger könnte sogar beginnen, selbst Schmutz zu produzieren, um mehr davon aufsaugen zu können. Dieses Phänomen wird, Reward Hacking, Manipulation der Belohnungspunktion genannt. Es ist sehr wichtig für Anwendungen, das richtige Leistungsmess zu entwickeln, das nicht durch den Agenten manipuliert werden kann.
