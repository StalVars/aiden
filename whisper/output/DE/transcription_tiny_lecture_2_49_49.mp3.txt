  Die am weitesten entwickelte Agent-Innenarchitektur sind zielbasierte Agent-Sternchen-Innen. Diese Agent-Innen setzen sich selbst Ziele, die sie in der Welt erreichen wollen. Dann nutzen sie ihr Weltmodell um Aktionen auszuwählen, die diese Ziele erreichen. Zielbasierte Agent-Innen unterschieden sich von modelbasierten Reflex-Agent-Innen nur in der Zielkomponente, welche die frühere einfache Bedingung-Aktion regel ersetzt. Man beachte, dass der Ziel orientierte Agent nach Aktionen sucht, um einen Zustand zu erreichen, indem das Ziel erfüllt ist. Allerdings ist jede Aktion-Sekwenz, die vom aktuellen Zustand zu einem Zielzustand führt, eine Lösung für den Agenten. Um zu unterscheiden, wie gut eine Aktion-Sekwenz oder eine einzelne Aktion ist, benötigt der Agent eine Nutzenfunktion, um den Nutzen verschiedene Zustände und Aktionen zu vergleichen oder um zwischen konkurrierenden Zielen zu wählen. Zielbasierte Agent-Innen lernen nicht, es sei denn, ihre Architektur wird um Komponenten von lernenden Agent-Innen ergänzt.
